{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/notebooks/blob/main/camenduru's_codeformer.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j47auBtDwHj4"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
        "!git clone https://huggingface.co/spaces/sczhou/CodeFormer\n",
        "%cd CodeFormer\n",
        "!pip install -qq -r requirements.txt\n",
        "!pip install -qq gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhBs6EKxwiiO"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('CodeFormer')\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gradio as gr\n",
        "\n",
        "from torchvision.transforms.functional import normalize\n",
        "\n",
        "from basicsr.utils import imwrite, img2tensor, tensor2img\n",
        "from basicsr.utils.download_util import load_file_from_url\n",
        "from facelib.utils.face_restoration_helper import FaceRestoreHelper\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from basicsr.utils.realesrgan_utils import RealESRGANer\n",
        "\n",
        "from basicsr.utils.registry import ARCH_REGISTRY\n",
        "\n",
        "\n",
        "os.system(\"pip freeze\")\n",
        "\n",
        "pretrain_model_url = {\n",
        "    'codeformer': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth',\n",
        "    'detection': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/detection_Resnet50_Final.pth',\n",
        "    'parsing': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/parsing_parsenet.pth',\n",
        "    'realesrgan': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/RealESRGAN_x2plus.pth'\n",
        "}\n",
        "# download weights\n",
        "if not os.path.exists('CodeFormer/weights/CodeFormer/codeformer.pth'):\n",
        "    load_file_from_url(url=pretrain_model_url['codeformer'], model_dir='CodeFormer/weights/CodeFormer', progress=True, file_name=None)\n",
        "if not os.path.exists('CodeFormer/weights/facelib/detection_Resnet50_Final.pth'):\n",
        "    load_file_from_url(url=pretrain_model_url['detection'], model_dir='CodeFormer/weights/facelib', progress=True, file_name=None)\n",
        "if not os.path.exists('CodeFormer/weights/facelib/parsing_parsenet.pth'):\n",
        "    load_file_from_url(url=pretrain_model_url['parsing'], model_dir='CodeFormer/weights/facelib', progress=True, file_name=None)\n",
        "if not os.path.exists('CodeFormer/weights/realesrgan/RealESRGAN_x2plus.pth'):\n",
        "    load_file_from_url(url=pretrain_model_url['realesrgan'], model_dir='CodeFormer/weights/realesrgan', progress=True, file_name=None)\n",
        "\n",
        "# download images\n",
        "torch.hub.download_url_to_file(\n",
        "    'https://replicate.com/api/models/sczhou/codeformer/files/fa3fe3d1-76b0-4ca8-ac0d-0a925cb0ff54/06.png',\n",
        "    '01.png')\n",
        "torch.hub.download_url_to_file(\n",
        "    'https://replicate.com/api/models/sczhou/codeformer/files/a1daba8e-af14-4b00-86a4-69cec9619b53/04.jpg',\n",
        "    '02.jpg')\n",
        "torch.hub.download_url_to_file(\n",
        "    'https://replicate.com/api/models/sczhou/codeformer/files/542d64f9-1712-4de7-85f7-3863009a7c3d/03.jpg',\n",
        "    '03.jpg')\n",
        "torch.hub.download_url_to_file(\n",
        "    'https://replicate.com/api/models/sczhou/codeformer/files/a11098b0-a18a-4c02-a19a-9a7045d68426/010.jpg',\n",
        "    '04.jpg')\n",
        "torch.hub.download_url_to_file(\n",
        "    'https://replicate.com/api/models/sczhou/codeformer/files/7cf19c2c-e0cf-4712-9af8-cf5bdbb8d0ee/012.jpg',\n",
        "    '05.jpg')\n",
        "\n",
        "def imread(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "# set enhancer with RealESRGAN\n",
        "def set_realesrgan():\n",
        "    half = True if torch.cuda.is_available() else False\n",
        "    model = RRDBNet(\n",
        "        num_in_ch=3,\n",
        "        num_out_ch=3,\n",
        "        num_feat=64,\n",
        "        num_block=23,\n",
        "        num_grow_ch=32,\n",
        "        scale=2,\n",
        "    )\n",
        "    upsampler = RealESRGANer(\n",
        "        scale=2,\n",
        "        model_path=\"CodeFormer/weights/realesrgan/RealESRGAN_x2plus.pth\",\n",
        "        model=model,\n",
        "        tile=400,\n",
        "        tile_pad=40,\n",
        "        pre_pad=0,\n",
        "        half=half,\n",
        "    )\n",
        "    return upsampler\n",
        "\n",
        "upsampler = set_realesrgan()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "codeformer_net = ARCH_REGISTRY.get(\"CodeFormer\")(\n",
        "    dim_embd=512,\n",
        "    codebook_size=1024,\n",
        "    n_head=8,\n",
        "    n_layers=9,\n",
        "    connect_list=[\"32\", \"64\", \"128\", \"256\"],\n",
        ").to(device)\n",
        "ckpt_path = \"CodeFormer/weights/CodeFormer/codeformer.pth\"\n",
        "checkpoint = torch.load(ckpt_path)[\"params_ema\"]\n",
        "codeformer_net.load_state_dict(checkpoint)\n",
        "codeformer_net.eval()\n",
        "\n",
        "os.makedirs('output', exist_ok=True)\n",
        "\n",
        "def inference(image, background_enhance, face_upsample, upscale, codeformer_fidelity):\n",
        "    \"\"\"Run a single prediction on the model\"\"\"\n",
        "    # take the default setting for the demo\n",
        "    has_aligned = False\n",
        "    only_center_face = False\n",
        "    draw_box = False\n",
        "    detection_model = \"retinaface_resnet50\"\n",
        "\n",
        "    face_helper = FaceRestoreHelper(\n",
        "        upscale,\n",
        "        face_size=512,\n",
        "        crop_ratio=(1, 1),\n",
        "        det_model=detection_model,\n",
        "        save_ext=\"png\",\n",
        "        use_parse=True,\n",
        "        device=device,\n",
        "    )\n",
        "    bg_upsampler = upsampler if background_enhance else None\n",
        "    face_upsampler = upsampler if face_upsample else None\n",
        "\n",
        "    img = cv2.imread(str(image), cv2.IMREAD_COLOR)\n",
        "\n",
        "    if has_aligned:\n",
        "        # the input faces are already cropped and aligned\n",
        "        img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
        "        face_helper.cropped_faces = [img]\n",
        "    else:\n",
        "        face_helper.read_image(img)\n",
        "        # get face landmarks for each face\n",
        "        num_det_faces = face_helper.get_face_landmarks_5(\n",
        "          only_center_face=only_center_face, resize=640, eye_dist_threshold=5\n",
        "        )\n",
        "        print(f\"\\tdetect {num_det_faces} faces\")\n",
        "        # align and warp each face\n",
        "        face_helper.align_warp_face()\n",
        "\n",
        "    # face restoration for each cropped face\n",
        "    for idx, cropped_face in enumerate(face_helper.cropped_faces):\n",
        "        # prepare data\n",
        "        cropped_face_t = img2tensor(\n",
        "            cropped_face / 255.0, bgr2rgb=True, float32=True\n",
        "        )\n",
        "        normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
        "        cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                output = codeformer_net(\n",
        "                    cropped_face_t, w=codeformer_fidelity, adain=True\n",
        "                )[0]\n",
        "                restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n",
        "            del output\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception as error:\n",
        "            print(f\"\\tFailed inference for CodeFormer: {error}\")\n",
        "            restored_face = tensor2img(\n",
        "                cropped_face_t, rgb2bgr=True, min_max=(-1, 1)\n",
        "            )\n",
        "\n",
        "        restored_face = restored_face.astype(\"uint8\")\n",
        "        face_helper.add_restored_face(restored_face)\n",
        "\n",
        "    # paste_back\n",
        "    if not has_aligned:\n",
        "        # upsample the background\n",
        "        if bg_upsampler is not None:\n",
        "            # Now only support RealESRGAN for upsampling background\n",
        "            bg_img = bg_upsampler.enhance(img, outscale=upscale)[0]\n",
        "        else:\n",
        "            bg_img = None\n",
        "        face_helper.get_inverse_affine(None)\n",
        "        # paste each restored face to the input image\n",
        "        if face_upsample and face_upsampler is not None:\n",
        "            restored_img = face_helper.paste_faces_to_input_image(\n",
        "                upsample_img=bg_img,\n",
        "                draw_box=draw_box,\n",
        "                face_upsampler=face_upsampler,\n",
        "            )\n",
        "        else:\n",
        "            restored_img = face_helper.paste_faces_to_input_image(\n",
        "                upsample_img=bg_img, draw_box=draw_box\n",
        "            )\n",
        "\n",
        "    # save restored img\n",
        "    save_path = f'output/out.png'\n",
        "    imwrite(restored_img, str(save_path))\n",
        "\n",
        "    restored_img = cv2.cvtColor(restored_img, cv2.COLOR_BGR2RGB)\n",
        "    return restored_img\n",
        "\n",
        "\n",
        "gr.Interface(\n",
        "    inference, [\n",
        "        gr.inputs.Image(type=\"filepath\", label=\"Input\"),\n",
        "        gr.inputs.Checkbox(default=True, label=\"Background_Enhance\"),\n",
        "        gr.inputs.Checkbox(default=True, label=\"Face_Upsample\"),\n",
        "        gr.inputs.Number(default=2, label=\"Rescaling_Factor\"),\n",
        "        gr.Slider(0, 1, value=0.5, step=0.01, label='Codeformer_Fidelity: 0 for better quality, 1 for better identity')\n",
        "    ], [\n",
        "        gr.outputs.Image(type=\"numpy\", label=\"Output\"),\n",
        "    ]\n",
        "    ).launch(inline=False, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
