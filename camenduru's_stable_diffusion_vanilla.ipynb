{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/notebooks/blob/main/camenduru's_stable_diffusion_vanilla.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKjG7_2uDP6g"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusion\"\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ela-xq-vDTTB"
      },
      "outputs": [],
      "source": [
        "!pip install -qq pytorch-lightning torch-fidelity numpy omegaconf einops kornia pytorch-lightning albumentations transformers ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "%cd stable-diffusion/\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./taming-transformers')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n",
        "\n",
        "!wget https://huggingface.co/camenduru/sd14/resolve/main/sd-v1-4.ckpt -O model.ckpt\n",
        "\n",
        "checkpoint_model_file = \"/content/stable-diffusion/model.ckpt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmRkvO7CDUmC"
      },
      "outputs": [],
      "source": [
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch.cuda.amp import autocast\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion.sampling import sample_lms\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'full' # 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "      \n",
        "def load_img(path, w, h):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize((w, h), Image.LANCZOS)\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "opt = config()\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "def generate(opt):\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    init_latent = None\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        init_image = load_img(opt.init_img, opt.W, opt.H).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code and init_latent == None:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "\n",
        "    images = []\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in range(opt.n_iter):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        if init_latent != None:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "                        else:\n",
        "\n",
        "                            if opt.sampler == 'klms':\n",
        "                                print(\"Using KLMS sampling\")\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                                model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                                x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                                samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                            else:\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                samples, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                                conditioning=c,\n",
        "                                                                batch_size=opt.n_samples,\n",
        "                                                                shape=shape,\n",
        "                                                                verbose=False,\n",
        "                                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                                unconditional_conditioning=uc,\n",
        "                                                                eta=opt.ddim_eta,\n",
        "                                                                x_T=start_code)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        for x_sample in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            filepath = os.path.join(opt.outdir, f\"{opt.seed}_{batch_idx}_{sample_idx:04}.png\")\n",
        "                            print(f\"Saving to {filepath}\")\n",
        "                            Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "                            sample_idx += 1\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehv_XQuHDWHP"
      },
      "outputs": [],
      "source": [
        "batch_name = \"013\"\n",
        "width_height = [512,512]\n",
        "guidance_scale = 7.5\n",
        "steps = 50\n",
        "samples_per_batch = 1 \n",
        "num_batch_images = 1000\n",
        "sampler = 'klms' #param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0\n",
        "seed = -1\n",
        "\n",
        "init_image_or_folder = \"\"\n",
        "init_strength = 0.0\n",
        "\n",
        "prompt = \"cat\"\n",
        "\n",
        "opt.init_img = init_image_or_folder\n",
        "opt.ddim_steps = steps\n",
        "opt.n_iter = 1\n",
        "opt.n_samples = samples_per_batch\n",
        "opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "opt.prompt = prompt\n",
        "opt.sampler = sampler\n",
        "opt.scale = guidance_scale\n",
        "opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "opt.W = width_height[0]\n",
        "opt.H = width_height[1]\n",
        "\n",
        "if opt.strength >= 1 or init_image_or_folder == None:\n",
        "    opt.init_img = \"\"\n",
        "\n",
        "if opt.init_img != None and opt.init_img != '':\n",
        "    opt.sampler = 'ddim'\n",
        "\n",
        "if opt.sampler != 'ddim':\n",
        "    opt.ddim_eta = 0.0\n",
        "\n",
        "settings = {\n",
        "    'ddim_eta': ddim_eta,\n",
        "    'guidance_scale': guidance_scale,\n",
        "    'init_image': init_image_or_folder,\n",
        "    'init_strength': init_strength,\n",
        "    'num_batch_images': num_batch_images,\n",
        "    'prompt': prompt,\n",
        "    'sampler': sampler,\n",
        "    'samples_per_batch': samples_per_batch,\n",
        "    'seed': opt.seed,\n",
        "    'steps': steps,\n",
        "    'width': opt.W,\n",
        "    'height': opt.H,\n",
        "}\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "    batch_idx += 1\n",
        "with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "sample_idx = 0\n",
        "\n",
        "for i in range(num_batch_images):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    images = generate(opt)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Used seed: {opt.seed}\")\n",
        "    print(f\"Saved to: {opt.outdir}\")\n",
        "    for image in images:\n",
        "        display(image)\n",
        "\n",
        "    opt.seed += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
