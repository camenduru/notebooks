{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/notebooks/blob/main/camenduru's_latent_diffusion_latex.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xxXZPslehgp8"
      },
      "outputs": [],
      "source": [
        "#@title Install & Download Model\n",
        "\n",
        "#from https://github.com/labmlai/annotated_deep_learning_paper_implementations\n",
        "!pip install -q transformers pytorch_lightning\n",
        "!wget https://huggingface.co/CompVis/stable-diffusion-v-1-1-original/resolve/main/sd-v1-1.ckpt -O /content/sd-v1-1.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "yanTNeiCivET"
      },
      "outputs": [],
      "source": [
        "#@title Autoencoder\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Autoencoder\n",
        "\n",
        "    This consists of the encoder and decoder modules.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder: 'Encoder', decoder: 'Decoder', emb_channels: int, z_channels: int):\n",
        "        \"\"\"\n",
        "        :param encoder: is the encoder\n",
        "        :param decoder: is the decoder\n",
        "        :param emb_channels: is the number of dimensions in the quantized embedding space\n",
        "        :param z_channels: is the number of channels in the embedding space\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        # Convolution to map from embedding space to\n",
        "        # quantized embedding space moments (mean and log variance)\n",
        "        self.quant_conv = nn.Conv2d(2 * z_channels, 2 * emb_channels, 1)\n",
        "        # Convolution to map from quantized embedding space back to\n",
        "        # embedding space\n",
        "        self.post_quant_conv = nn.Conv2d(emb_channels, z_channels, 1)\n",
        "\n",
        "    def encode(self, img: torch.Tensor) -> 'GaussianDistribution':\n",
        "        \"\"\"\n",
        "        ### Encode images to latent representation\n",
        "\n",
        "        :param img: is the image tensor with shape `[batch_size, img_channels, img_height, img_width]`\n",
        "        \"\"\"\n",
        "        # Get embeddings with shape `[batch_size, z_channels * 2, z_height, z_height]`\n",
        "        z = self.encoder(img)\n",
        "        # Get the moments in the quantized embedding space\n",
        "        moments = self.quant_conv(z)\n",
        "        # Return the distribution\n",
        "        return GaussianDistribution(moments)\n",
        "\n",
        "    def decode(self, z: torch.Tensor):\n",
        "        \"\"\"\n",
        "        ### Decode images from latent representation\n",
        "\n",
        "        :param z: is the latent representation with shape `[batch_size, emb_channels, z_height, z_height]`\n",
        "        \"\"\"\n",
        "        # Map to embedding space from the quantized representation\n",
        "        z = self.post_quant_conv(z)\n",
        "        # Decode the image of shape `[batch_size, channels, height, width]`\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Encoder module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, channels: int, channel_multipliers: List[int], n_resnet_blocks: int,\n",
        "                 in_channels: int, z_channels: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels in the first convolution layer\n",
        "        :param channel_multipliers: are the multiplicative factors for the number of channels in the\n",
        "            subsequent blocks\n",
        "        :param n_resnet_blocks: is the number of resnet layers at each resolution\n",
        "        :param in_channels: is the number of channels in the image\n",
        "        :param z_channels: is the number of channels in the embedding space\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of blocks of different resolutions.\n",
        "        # The resolution is halved at the end each top level block\n",
        "        n_resolutions = len(channel_multipliers)\n",
        "\n",
        "        # Initial $3 \\times 3$ convolution layer that maps the image to `channels`\n",
        "        self.conv_in = nn.Conv2d(in_channels, channels, 3, stride=1, padding=1)\n",
        "\n",
        "        # Number of channels in each top level block\n",
        "        channels_list = [m * channels for m in [1] + channel_multipliers]\n",
        "\n",
        "        # List of top-level blocks\n",
        "        self.down = nn.ModuleList()\n",
        "        # Create top-level blocks\n",
        "        for i in range(n_resolutions):\n",
        "            # Each top level block consists of multiple ResNet Blocks and down-sampling\n",
        "            resnet_blocks = nn.ModuleList()\n",
        "            # Add ResNet Blocks\n",
        "            for _ in range(n_resnet_blocks):\n",
        "                resnet_blocks.append(ResnetBlock(channels, channels_list[i + 1]))\n",
        "                channels = channels_list[i + 1]\n",
        "            # Top-level block\n",
        "            down = nn.Module()\n",
        "            down.block = resnet_blocks\n",
        "            # Down-sampling at the end of each top level block except the last\n",
        "            if i != n_resolutions - 1:\n",
        "                down.downsample = DownSample(channels)\n",
        "            else:\n",
        "                down.downsample = nn.Identity()\n",
        "            #\n",
        "            self.down.append(down)\n",
        "\n",
        "        # Final ResNet blocks with attention\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(channels, channels)\n",
        "        self.mid.attn_1 = AttnBlock(channels)\n",
        "        self.mid.block_2 = ResnetBlock(channels, channels)\n",
        "\n",
        "        # Map to embedding space with a $3 \\times 3$ convolution\n",
        "        self.norm_out = normalization(channels)\n",
        "        self.conv_out = nn.Conv2d(channels, 2 * z_channels, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, img: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param img: is the image tensor with shape `[batch_size, img_channels, img_height, img_width]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Map to `channels` with the initial convolution\n",
        "        x = self.conv_in(img)\n",
        "\n",
        "        # Top-level blocks\n",
        "        for down in self.down:\n",
        "            # ResNet Blocks\n",
        "            for block in down.block:\n",
        "                x = block(x)\n",
        "            # Down-sampling\n",
        "            x = down.downsample(x)\n",
        "\n",
        "        # Final ResNet blocks with attention\n",
        "        x = self.mid.block_1(x)\n",
        "        x = self.mid.attn_1(x)\n",
        "        x = self.mid.block_2(x)\n",
        "\n",
        "        # Normalize and map to embedding space\n",
        "        x = self.norm_out(x)\n",
        "        x = swish(x)\n",
        "        x = self.conv_out(x)\n",
        "\n",
        "        #\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Decoder module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, channels: int, channel_multipliers: List[int], n_resnet_blocks: int,\n",
        "                 out_channels: int, z_channels: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels in the final convolution layer\n",
        "        :param channel_multipliers: are the multiplicative factors for the number of channels in the\n",
        "            previous blocks, in reverse order\n",
        "        :param n_resnet_blocks: is the number of resnet layers at each resolution\n",
        "        :param out_channels: is the number of channels in the image\n",
        "        :param z_channels: is the number of channels in the embedding space\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of blocks of different resolutions.\n",
        "        # The resolution is halved at the end each top level block\n",
        "        num_resolutions = len(channel_multipliers)\n",
        "\n",
        "        # Number of channels in each top level block, in the reverse order\n",
        "        channels_list = [m * channels for m in channel_multipliers]\n",
        "\n",
        "        # Number of channels in the  top-level block\n",
        "        channels = channels_list[-1]\n",
        "\n",
        "        # Initial $3 \\times 3$ convolution layer that maps the embedding space to `channels`\n",
        "        self.conv_in = nn.Conv2d(z_channels, channels, 3, stride=1, padding=1)\n",
        "\n",
        "        # ResNet blocks with attention\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(channels, channels)\n",
        "        self.mid.attn_1 = AttnBlock(channels)\n",
        "        self.mid.block_2 = ResnetBlock(channels, channels)\n",
        "\n",
        "        # List of top-level blocks\n",
        "        self.up = nn.ModuleList()\n",
        "        # Create top-level blocks\n",
        "        for i in reversed(range(num_resolutions)):\n",
        "            # Each top level block consists of multiple ResNet Blocks and up-sampling\n",
        "            resnet_blocks = nn.ModuleList()\n",
        "            # Add ResNet Blocks\n",
        "            for _ in range(n_resnet_blocks + 1):\n",
        "                resnet_blocks.append(ResnetBlock(channels, channels_list[i]))\n",
        "                channels = channels_list[i]\n",
        "            # Top-level block\n",
        "            up = nn.Module()\n",
        "            up.block = resnet_blocks\n",
        "            # Up-sampling at the end of each top level block except the first\n",
        "            if i != 0:\n",
        "                up.upsample = UpSample(channels)\n",
        "            else:\n",
        "                up.upsample = nn.Identity()\n",
        "            # Prepend to be consistent with the checkpoint\n",
        "            self.up.insert(0, up)\n",
        "\n",
        "        # Map to image space with a $3 \\times 3$ convolution\n",
        "        self.norm_out = normalization(channels)\n",
        "        self.conv_out = nn.Conv2d(channels, out_channels, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param z: is the embedding tensor with shape `[batch_size, z_channels, z_height, z_height]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Map to `channels` with the initial convolution\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # ResNet blocks with attention\n",
        "        h = self.mid.block_1(h)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h)\n",
        "\n",
        "        # Top-level blocks\n",
        "        for up in reversed(self.up):\n",
        "            # ResNet Blocks\n",
        "            for block in up.block:\n",
        "                h = block(h)\n",
        "            # Up-sampling\n",
        "            h = up.upsample(h)\n",
        "\n",
        "        # Normalize and map to image space\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        img = self.conv_out(h)\n",
        "\n",
        "        #\n",
        "        return img\n",
        "\n",
        "\n",
        "class GaussianDistribution:\n",
        "    \"\"\"\n",
        "    ## Gaussian Distribution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parameters: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param parameters: are the means and log of variances of the embedding of shape\n",
        "            `[batch_size, z_channels * 2, z_height, z_height]`\n",
        "        \"\"\"\n",
        "        # Split mean and log of variance\n",
        "        self.mean, log_var = torch.chunk(parameters, 2, dim=1)\n",
        "        # Clamp the log of variances\n",
        "        self.log_var = torch.clamp(log_var, -30.0, 20.0)\n",
        "        # Calculate standard deviation\n",
        "        self.std = torch.exp(0.5 * self.log_var)\n",
        "\n",
        "    def sample(self):\n",
        "        # Sample from the distribution\n",
        "        return self.mean + self.std * torch.randn_like(self.std)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Attention block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Group normalization\n",
        "        self.norm = normalization(channels)\n",
        "        # Query, key and value mappings\n",
        "        self.q = nn.Conv2d(channels, channels, 1)\n",
        "        self.k = nn.Conv2d(channels, channels, 1)\n",
        "        self.v = nn.Conv2d(channels, channels, 1)\n",
        "        # Final $1 \\times 1$ convolution layer\n",
        "        self.proj_out = nn.Conv2d(channels, channels, 1)\n",
        "        # Attention scaling factor\n",
        "        self.scale = channels ** -0.5\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the tensor of shape `[batch_size, channels, height, width]`\n",
        "        \"\"\"\n",
        "        # Normalize `x`\n",
        "        x_norm = self.norm(x)\n",
        "        # Get query, key and vector embeddings\n",
        "        q = self.q(x_norm)\n",
        "        k = self.k(x_norm)\n",
        "        v = self.v(x_norm)\n",
        "\n",
        "        # Reshape to query, key and vector embeedings from\n",
        "        # `[batch_size, channels, height, width]` to\n",
        "        # `[batch_size, channels, height * width]`\n",
        "        b, c, h, w = q.shape\n",
        "        q = q.view(b, c, h * w)\n",
        "        k = k.view(b, c, h * w)\n",
        "        v = v.view(b, c, h * w)\n",
        "\n",
        "        # Compute $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)$\n",
        "        attn = torch.einsum('bci,bcj->bij', q, k) * self.scale\n",
        "        attn = F.softmax(attn, dim=2)\n",
        "\n",
        "        # Compute $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)V$\n",
        "        out = torch.einsum('bij,bcj->bci', attn, v)\n",
        "\n",
        "        # Reshape back to `[batch_size, channels, height, width]`\n",
        "        out = out.view(b, c, h, w)\n",
        "        # Final $1 \\times 1$ convolution layer\n",
        "        out = self.proj_out(out)\n",
        "\n",
        "        # Add residual connection\n",
        "        return x + out\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Up-sampling layer\n",
        "    \"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # $3 \\times 3$ convolution mapping\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n",
        "        \"\"\"\n",
        "        # Up-sample by a factor of $2$\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        # Apply convolution\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Down-sampling layer\n",
        "    \"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # $3 \\times 3$ convolution with stride length of $2$ to down-sample by a factor of $2$\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n",
        "        \"\"\"\n",
        "        # Add padding\n",
        "        x = F.pad(x, (0, 1, 0, 1), mode=\"constant\", value=0)\n",
        "        # Apply convolution\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ## ResNet Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        :param in_channels: is the number of channels in the input\n",
        "        :param out_channels: is the number of channels in the output\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # First normalization and convolution layer\n",
        "        self.norm1 = normalization(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1)\n",
        "        # Second normalization and convolution layer\n",
        "        self.norm2 = normalization(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
        "        # `in_channels` to `out_channels` mapping layer for residual connection\n",
        "        if in_channels != out_channels:\n",
        "            self.nin_shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0)\n",
        "        else:\n",
        "            self.nin_shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n",
        "        \"\"\"\n",
        "\n",
        "        h = x\n",
        "\n",
        "        # First normalization and convolution layer\n",
        "        h = self.norm1(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        # Second normalization and convolution layer\n",
        "        h = self.norm2(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        # Map and add residual\n",
        "        return self.nin_shortcut(x) + h\n",
        "\n",
        "\n",
        "def swish(x: torch.Tensor):\n",
        "    \"\"\"\n",
        "    ### Swish activation\n",
        "\n",
        "    $$x \\cdot \\sigma(x)$$\n",
        "    \"\"\"\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def normalization(channels: int):\n",
        "    \"\"\"\n",
        "    ### Group normalization\n",
        "\n",
        "    This is a helper function, with fixed number of groups and `eps`.\n",
        "    \"\"\"\n",
        "    return nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "tJdi-CNei7RX"
      },
      "outputs": [],
      "source": [
        "#@title CLIPTextEmbedder\n",
        "\n",
        "from typing import List\n",
        "\n",
        "from torch import nn\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "\n",
        "class CLIPTextEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    ## CLIP Text Embedder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, version: str = \"openai/clip-vit-large-patch14\", device=\"cuda:0\", max_length: int = 77):\n",
        "        \"\"\"\n",
        "        :param version: is the model version\n",
        "        :param device: is the device\n",
        "        :param max_length: is the max length of the tokenized prompt\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Load the tokenizer\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
        "        # Load the CLIP transformer\n",
        "        self.transformer = CLIPTextModel.from_pretrained(version).eval()\n",
        "\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def forward(self, prompts: List[str]):\n",
        "        \"\"\"\n",
        "        :param prompts: are the list of prompts to embed\n",
        "        \"\"\"\n",
        "        # Tokenize the prompts\n",
        "        batch_encoding = self.tokenizer(prompts, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        # Get token ids\n",
        "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
        "        # Get CLIP embeddings\n",
        "        return self.transformer(input_ids=tokens).last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "LWfE5L3hjzS0"
      },
      "outputs": [],
      "source": [
        "#@title SpatialTransformer | CrossAttention\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Spatial Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, n_heads: int, n_layers: int, d_cond: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels in the feature map\n",
        "        :param n_heads: is the number of attention heads\n",
        "        :param n_layers: is the number of transformer layers\n",
        "        :param d_cond: is the size of the conditional embedding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Initial group normalization\n",
        "        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n",
        "        # Initial $1 \\times 1$ convolution\n",
        "        self.proj_in = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(channels, n_heads, channels // n_heads, d_cond=d_cond) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        # Final $1 \\times 1$ convolution\n",
        "        self.proj_out = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the feature map of shape `[batch_size, channels, height, width]`\n",
        "        :param cond: is the conditional embeddings of shape `[batch_size,  n_cond, d_cond]`\n",
        "        \"\"\"\n",
        "        # Get shape `[batch_size, channels, height, width]`\n",
        "        b, c, h, w = x.shape\n",
        "        # For residual connection\n",
        "        x_in = x\n",
        "        # Normalize\n",
        "        x = self.norm(x)\n",
        "        # Initial $1 \\times 1$ convolution\n",
        "        x = self.proj_in(x)\n",
        "        # Transpose and reshape from `[batch_size, channels, height, width]`\n",
        "        # to `[batch_size, height * width, channels]`\n",
        "        x = x.permute(0, 2, 3, 1).view(b, h * w, c)\n",
        "        # Apply the transformer layers\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, cond)\n",
        "        # Reshape and transpose from `[batch_size, height * width, channels]`\n",
        "        # to `[batch_size, channels, height, width]`\n",
        "        x = x.view(b, h, w, c).permute(0, 3, 1, 2)\n",
        "        # Final $1 \\times 1$ convolution\n",
        "        x = self.proj_out(x)\n",
        "        # Add residual\n",
        "        return x + x_in\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Transformer Layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, n_heads: int, d_head: int, d_cond: int):\n",
        "        \"\"\"\n",
        "        :param d_model: is the input embedding size\n",
        "        :param n_heads: is the number of attention heads\n",
        "        :param d_head: is the size of a attention head\n",
        "        :param d_cond: is the size of the conditional embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Self-attention layer and pre-norm layer\n",
        "        self.attn1 = CrossAttention(d_model, d_model, n_heads, d_head)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        # Cross attention layer and pre-norm layer\n",
        "        self.attn2 = CrossAttention(d_model, d_cond, n_heads, d_head)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        # Feed-forward network and pre-norm layer\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: are the input embeddings of shape `[batch_size, height * width, d_model]`\n",
        "        :param cond: is the conditional embeddings of shape `[batch_size,  n_cond, d_cond]`\n",
        "        \"\"\"\n",
        "        # Self attention\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        # Cross-attention with conditioning\n",
        "        x = self.attn2(self.norm2(x), cond=cond) + x\n",
        "        # Feed-forward network\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        #\n",
        "        return x\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Cross Attention Layer\n",
        "\n",
        "    This falls-back to self-attention when conditional embeddings are not specified.\n",
        "    \"\"\"\n",
        "\n",
        "    use_flash_attention: bool = False\n",
        "\n",
        "    def __init__(self, d_model: int, d_cond: int, n_heads: int, d_head: int, is_inplace: bool = True):\n",
        "        \"\"\"\n",
        "        :param d_model: is the input embedding size\n",
        "        :param n_heads: is the number of attention heads\n",
        "        :param d_head: is the size of a attention head\n",
        "        :param d_cond: is the size of the conditional embeddings\n",
        "        :param is_inplace: specifies whether to perform the attention softmax computation inplace to\n",
        "            save memory\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.is_inplace = is_inplace\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_head\n",
        "\n",
        "        # Attention scaling factor\n",
        "        self.scale = d_head ** -0.5\n",
        "\n",
        "        # Query, key and value mappings\n",
        "        d_attn = d_head * n_heads\n",
        "        self.to_q = nn.Linear(d_model, d_attn, bias=False)\n",
        "        self.to_k = nn.Linear(d_cond, d_attn, bias=False)\n",
        "        self.to_v = nn.Linear(d_cond, d_attn, bias=False)\n",
        "\n",
        "        # Final linear layer\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_attn, d_model))\n",
        "\n",
        "        # Setup [flash attention](https://github.com/HazyResearch/flash-attention).\n",
        "        # Flash attention is only used if it's installed\n",
        "        # and `CrossAttention.use_flash_attention` is set to `True`.\n",
        "        try:\n",
        "            # You can install flash attention by cloning their Github repo,\n",
        "            # [https://github.com/HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)\n",
        "            # and then running `python setup.py install`\n",
        "            from flash_attn.flash_attention import FlashAttention\n",
        "            self.flash = FlashAttention()\n",
        "            # Set the scale for scaled dot-product attention.\n",
        "            self.flash.softmax_scale = self.scale\n",
        "        # Set to `None` if it's not installed\n",
        "        except ImportError:\n",
        "            self.flash = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cond: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        :param x: are the input embeddings of shape `[batch_size, height * width, d_model]`\n",
        "        :param cond: is the conditional embeddings of shape `[batch_size, n_cond, d_cond]`\n",
        "        \"\"\"\n",
        "\n",
        "        # If `cond` is `None` we perform self attention\n",
        "        has_cond = cond is not None\n",
        "        if not has_cond:\n",
        "            cond = x\n",
        "\n",
        "        # Get query, key and value vectors\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(cond)\n",
        "        v = self.to_v(cond)\n",
        "\n",
        "        # Use flash attention if it's available and the head size is less than or equal to `128`\n",
        "        if CrossAttention.use_flash_attention and self.flash is not None and not has_cond and self.d_head <= 128:\n",
        "            return self.flash_attention(q, k, v)\n",
        "        # Otherwise, fallback to normal attention\n",
        "        else:\n",
        "            return self.normal_attention(q, k, v)\n",
        "\n",
        "    def flash_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
        "        \"\"\"\n",
        "        #### Flash Attention\n",
        "\n",
        "        :param q: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n",
        "        :param k: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n",
        "        :param v: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size and number of elements along sequence axis (`width * height`)\n",
        "        batch_size, seq_len, _ = q.shape\n",
        "\n",
        "        # Stack `q`, `k`, `v` vectors for flash attention, to get a single tensor of\n",
        "        # shape `[batch_size, seq_len, 3, n_heads * d_head]`\n",
        "        qkv = torch.stack((q, k, v), dim=2)\n",
        "        # Split the heads\n",
        "        qkv = qkv.view(batch_size, seq_len, 3, self.n_heads, self.d_head)\n",
        "\n",
        "        # Flash attention works for head sizes `32`, `64` and `128`, so we have to pad the heads to\n",
        "        # fit this size.\n",
        "        if self.d_head <= 32:\n",
        "            pad = 32 - self.d_head\n",
        "        elif self.d_head <= 64:\n",
        "            pad = 64 - self.d_head\n",
        "        elif self.d_head <= 128:\n",
        "            pad = 128 - self.d_head\n",
        "        else:\n",
        "            raise ValueError(f'Head size ${self.d_head} too large for Flash Attention')\n",
        "\n",
        "        # Pad the heads\n",
        "        if pad:\n",
        "            qkv = torch.cat((qkv, qkv.new_zeros(batch_size, seq_len, 3, self.n_heads, pad)), dim=-1)\n",
        "\n",
        "        # Compute attention\n",
        "        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)V$$\n",
        "        # This gives a tensor of shape `[batch_size, seq_len, n_heads, d_padded]`\n",
        "        out, _ = self.flash(qkv)\n",
        "        # Truncate the extra head size\n",
        "        out = out[:, :, :, :self.d_head]\n",
        "        # Reshape to `[batch_size, seq_len, n_heads * d_head]`\n",
        "        out = out.reshape(batch_size, seq_len, self.n_heads * self.d_head)\n",
        "\n",
        "        # Map to `[batch_size, height * width, d_model]` with a linear layer\n",
        "        return self.to_out(out)\n",
        "\n",
        "    def normal_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
        "        \"\"\"\n",
        "        #### Normal Attention\n",
        "\n",
        "        :param q: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n",
        "        :param k: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n",
        "        :param v: are the query vectors before splitting heads, of shape `[batch_size, seq, d_attn]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Split them to heads of shape `[batch_size, seq_len, n_heads, d_head]`\n",
        "        q = q.view(*q.shape[:2], self.n_heads, -1)\n",
        "        k = k.view(*k.shape[:2], self.n_heads, -1)\n",
        "        v = v.view(*v.shape[:2], self.n_heads, -1)\n",
        "\n",
        "        # Calculate attention $\\frac{Q K^\\top}{\\sqrt{d_{key}}}$\n",
        "        attn = torch.einsum('bihd,bjhd->bhij', q, k) * self.scale\n",
        "\n",
        "        # Compute softmax\n",
        "        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)$$\n",
        "        if self.is_inplace:\n",
        "            half = attn.shape[0] // 2\n",
        "            attn[half:] = attn[half:].softmax(dim=-1)\n",
        "            attn[:half] = attn[:half].softmax(dim=-1)\n",
        "        else:\n",
        "            attn = attn.softmax(dim=-1)\n",
        "\n",
        "        # Compute attention output\n",
        "        # $$\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_{key}}}\\Bigg)V$$\n",
        "        out = torch.einsum('bhij,bjhd->bihd', attn, v)\n",
        "        # Reshape to `[batch_size, height * width, n_heads * d_head]`\n",
        "        out = out.reshape(*out.shape[:2], -1)\n",
        "        # Map to `[batch_size, height * width, d_model]` with a linear layer\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Feed-Forward Network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, d_mult: int = 4):\n",
        "        \"\"\"\n",
        "        :param d_model: is the input embedding size\n",
        "        :param d_mult: is multiplicative factor for the hidden layer size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            GeGLU(d_model, d_model * d_mult),\n",
        "            nn.Dropout(0.),\n",
        "            nn.Linear(d_model * d_mult, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class GeGLU(nn.Module):\n",
        "    \"\"\"\n",
        "    ### GeGLU Activation\n",
        "\n",
        "    $$\\text{GeGLU}(x) = (xW + b) * \\text{GELU}(xV + c)$$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_in: int, d_out: int):\n",
        "        super().__init__()\n",
        "        # Combined linear projections $xW + b$ and $xV + c$\n",
        "        self.proj = nn.Linear(d_in, d_out * 2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Get $xW + b$ and $xV + c$\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        # $\\text{GeGLU}(x) = (xW + b) * \\text{GELU}(xV + c)$\n",
        "        return x * F.gelu(gate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "HQejRsDFi_y7"
      },
      "outputs": [],
      "source": [
        "#@title UNetModel\n",
        "\n",
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    ## U-Net model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self, *,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            channels: int,\n",
        "            n_res_blocks: int,\n",
        "            attention_levels: List[int],\n",
        "            channel_multipliers: List[int],\n",
        "            n_heads: int,\n",
        "            tf_layers: int = 1,\n",
        "            d_cond: int = 768):\n",
        "        \"\"\"\n",
        "        :param in_channels: is the number of channels in the input feature map\n",
        "        :param out_channels: is the number of channels in the output feature map\n",
        "        :param channels: is the base channel count for the model\n",
        "        :param n_res_blocks: number of residual blocks at each level\n",
        "        :param attention_levels: are the levels at which attention should be performed\n",
        "        :param channel_multipliers: are the multiplicative factors for number of channels for each level\n",
        "        :param n_heads: is the number of attention heads in the transformers\n",
        "        :param tf_layers: is the number of transformer layers in the transformers\n",
        "        :param d_cond: is the size of the conditional embedding in the transformers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "\n",
        "        # Number of levels\n",
        "        levels = len(channel_multipliers)\n",
        "        # Size time embeddings\n",
        "        d_time_emb = channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(channels, d_time_emb),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(d_time_emb, d_time_emb),\n",
        "        )\n",
        "\n",
        "        # Input half of the U-Net\n",
        "        self.input_blocks = nn.ModuleList()\n",
        "        # Initial $3 \\times 3$ convolution that maps the input to `channels`.\n",
        "        # The blocks are wrapped in `TimestepEmbedSequential` module because\n",
        "        # different modules have different forward function signatures;\n",
        "        # for example, convolution only accepts the feature map and\n",
        "        # residual blocks accept the feature map and time embedding.\n",
        "        # `TimestepEmbedSequential` calls them accordingly.\n",
        "        self.input_blocks.append(TimestepEmbedSequential(\n",
        "            nn.Conv2d(in_channels, channels, 3, padding=1)))\n",
        "        # Number of channels at each block in the input half of U-Net\n",
        "        input_block_channels = [channels]\n",
        "        # Number of channels at each level\n",
        "        channels_list = [channels * m for m in channel_multipliers]\n",
        "        # Prepare levels\n",
        "        for i in range(levels):\n",
        "            # Add the residual blocks and attentions\n",
        "            for _ in range(n_res_blocks):\n",
        "                # Residual block maps from previous number of channels to the number of\n",
        "                # channels in the current level\n",
        "                layers = [ResBlock(channels, d_time_emb, out_channels=channels_list[i])]\n",
        "                channels = channels_list[i]\n",
        "                # Add transformer\n",
        "                if i in attention_levels:\n",
        "                    layers.append(SpatialTransformer(channels, n_heads, tf_layers, d_cond))\n",
        "                # Add them to the input half of the U-Net and keep track of the number of channels of\n",
        "                # its output\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                input_block_channels.append(channels)\n",
        "            # Down sample at all levels except last\n",
        "            if i != levels - 1:\n",
        "                self.input_blocks.append(TimestepEmbedSequential(DownSample(channels)))\n",
        "                input_block_channels.append(channels)\n",
        "\n",
        "        # The middle of the U-Net\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(channels, d_time_emb),\n",
        "            SpatialTransformer(channels, n_heads, tf_layers, d_cond),\n",
        "            ResBlock(channels, d_time_emb),\n",
        "        )\n",
        "\n",
        "        # Second half of the U-Net\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        # Prepare levels in reverse order\n",
        "        for i in reversed(range(levels)):\n",
        "            # Add the residual blocks and attentions\n",
        "            for j in range(n_res_blocks + 1):\n",
        "                # Residual block maps from previous number of channels plus the\n",
        "                # skip connections from the input half of U-Net to the number of\n",
        "                # channels in the current level.\n",
        "                layers = [ResBlock(channels + input_block_channels.pop(), d_time_emb, out_channels=channels_list[i])]\n",
        "                channels = channels_list[i]\n",
        "                # Add transformer\n",
        "                if i in attention_levels:\n",
        "                    layers.append(SpatialTransformer(channels, n_heads, tf_layers, d_cond))\n",
        "                # Up-sample at every level after last residual block\n",
        "                # except the last one.\n",
        "                # Note that we are iterating in reverse; i.e. `i == 0` is the last.\n",
        "                if i != 0 and j == n_res_blocks:\n",
        "                    layers.append(UpSample(channels))\n",
        "                # Add to the output half of the U-Net\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "\n",
        "        # Final normalization and $3 \\times 3$ convolution\n",
        "        self.out = nn.Sequential(\n",
        "            normalization(channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels, out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def time_step_embedding(self, time_steps: torch.Tensor, max_period: int = 10000):\n",
        "        \"\"\"\n",
        "        ## Create sinusoidal time step embeddings\n",
        "\n",
        "        :param time_steps: are the time steps of shape `[batch_size]`\n",
        "        :param max_period: controls the minimum frequency of the embeddings.\n",
        "        \"\"\"\n",
        "        # $\\frac{c}{2}$; half the channels are sin and the other half is cos,\n",
        "        half = self.channels // 2\n",
        "        # $\\frac{1}{10000^{\\frac{2i}{c}}}$\n",
        "        frequencies = torch.exp(\n",
        "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "        ).to(device=time_steps.device)\n",
        "        # $\\frac{t}{10000^{\\frac{2i}{c}}}$\n",
        "        args = time_steps[:, None].float() * frequencies[None]\n",
        "        # $\\cos\\Bigg(\\frac{t}{10000^{\\frac{2i}{c}}}\\Bigg)$ and $\\sin\\Bigg(\\frac{t}{10000^{\\frac{2i}{c}}}\\Bigg)$\n",
        "        return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, time_steps: torch.Tensor, cond: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the input feature map of shape `[batch_size, channels, width, height]`\n",
        "        :param time_steps: are the time steps of shape `[batch_size]`\n",
        "        :param cond: conditioning of shape `[batch_size, n_cond, d_cond]`\n",
        "        \"\"\"\n",
        "        # To store the input half outputs for skip connections\n",
        "        x_input_block = []\n",
        "\n",
        "        # Get time step embeddings\n",
        "        t_emb = self.time_step_embedding(time_steps)\n",
        "        t_emb = self.time_embed(t_emb)\n",
        "\n",
        "        # Input half of the U-Net\n",
        "        for module in self.input_blocks:\n",
        "            x = module(x, t_emb, cond)\n",
        "            x_input_block.append(x)\n",
        "        # Middle of the U-Net\n",
        "        x = self.middle_block(x, t_emb, cond)\n",
        "        # Output half of the U-Net\n",
        "        for module in self.output_blocks:\n",
        "            x = torch.cat([x, x_input_block.pop()], dim=1)\n",
        "            x = module(x, t_emb, cond)\n",
        "\n",
        "        # Final normalization and $3 \\times 3$ convolution\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential):\n",
        "    \"\"\"\n",
        "    ### Sequential block for modules with different inputs\n",
        "\n",
        "    This sequential module can compose of different modules such as `ResBlock`,\n",
        "    `nn.Conv` and `SpatialTransformer` and calls them with the matching signatures\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x, t_emb, cond=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, ResBlock):\n",
        "                x = layer(x, t_emb)\n",
        "            elif isinstance(layer, SpatialTransformer):\n",
        "                x = layer(x, cond)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Up-sampling layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # $3 \\times 3$ convolution mapping\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n",
        "        \"\"\"\n",
        "        # Up-sample by a factor of $2$\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        # Apply convolution\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Down-sampling layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int):\n",
        "        \"\"\"\n",
        "        :param channels: is the number of channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # $3 \\times 3$ convolution with stride length of $2$ to down-sample by a factor of $2$\n",
        "        self.op = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n",
        "        \"\"\"\n",
        "        # Apply convolution\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ## ResNet Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, d_t_emb: int, *, out_channels=None):\n",
        "        \"\"\"\n",
        "        :param channels: the number of input channels\n",
        "        :param d_t_emb: the size of timestep embeddings\n",
        "        :param out_channels: is the number of out channels. defaults to `channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # `out_channels` not specified\n",
        "        if out_channels is None:\n",
        "            out_channels = channels\n",
        "\n",
        "        # First normalization and convolution\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization(channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels, out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        # Time step embeddings\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(d_t_emb, out_channels),\n",
        "        )\n",
        "        # Final convolution layer\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization(out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "        # `channels` to `out_channels` mapping layer for residual connection\n",
        "        if out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        else:\n",
        "            self.skip_connection = nn.Conv2d(channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: is the input feature map with shape `[batch_size, channels, height, width]`\n",
        "        :param t_emb: is the time step embeddings of shape `[batch_size, d_t_emb]`\n",
        "        \"\"\"\n",
        "        # Initial convolution\n",
        "        h = self.in_layers(x)\n",
        "        # Time step embeddings\n",
        "        t_emb = self.emb_layers(t_emb).type(h.dtype)\n",
        "        # Add time step embeddings\n",
        "        h = h + t_emb[:, :, None, None]\n",
        "        # Final convolution\n",
        "        h = self.out_layers(h)\n",
        "        # Add skip connection\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    \"\"\"\n",
        "    ### Group normalization with float32 casting\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "\n",
        "def normalization(channels):\n",
        "    \"\"\"\n",
        "    ### Group normalization\n",
        "\n",
        "    This is a helper function, with fixed number of groups..\n",
        "    \"\"\"\n",
        "    return GroupNorm32(32, channels)\n",
        "\n",
        "\n",
        "def _test_time_embeddings():\n",
        "    \"\"\"\n",
        "    Test sinusoidal time step embeddings\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    m = UNetModel(in_channels=1, out_channels=1, channels=320, n_res_blocks=1, attention_levels=[],\n",
        "                  channel_multipliers=[],\n",
        "                  n_heads=1, tf_layers=1, d_cond=1)\n",
        "    te = m.time_step_embedding(torch.arange(0, 1000))\n",
        "    plt.plot(np.arange(1000), te[:, [50, 100, 190, 260]].numpy())\n",
        "    plt.legend([\"dim %d\" % p for p in [50, 100, 190, 260]])\n",
        "    plt.title(\"Time embeddings\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#\n",
        "# if __name__ == '__main__':\n",
        "#     _test_time_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "3f_oamxVidgE"
      },
      "outputs": [],
      "source": [
        "#@title LatentDiffusion\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DiffusionWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    *This is an empty wrapper class around the [U-Net](model/unet.html).\n",
        "    We keep this to have the same model structure as\n",
        "    [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)\n",
        "    so that we do not have to map the checkpoint weights explicitly*.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, diffusion_model: UNetModel):\n",
        "        super().__init__()\n",
        "        self.diffusion_model = diffusion_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor, time_steps: torch.Tensor, context: torch.Tensor):\n",
        "        return self.diffusion_model(x, time_steps, context)\n",
        "\n",
        "\n",
        "class LatentDiffusion(nn.Module):\n",
        "    \"\"\"\n",
        "    ## Latent diffusion model\n",
        "\n",
        "    This contains following components:\n",
        "\n",
        "    * [AutoEncoder](model/autoencoder.html)\n",
        "    * [U-Net](model/unet.html) with [attention](model/unet_attention.html)\n",
        "    * [CLIP embeddings generator](model/clip_embedder.html)\n",
        "    \"\"\"\n",
        "    model: DiffusionWrapper\n",
        "    first_stage_model: Autoencoder\n",
        "    cond_stage_model: CLIPTextEmbedder\n",
        "\n",
        "    def __init__(self,\n",
        "                 unet_model: UNetModel,\n",
        "                 autoencoder: Autoencoder,\n",
        "                 clip_embedder: CLIPTextEmbedder,\n",
        "                 latent_scaling_factor: float,\n",
        "                 n_steps: int,\n",
        "                 linear_start: float,\n",
        "                 linear_end: float,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param unet_model: is the [U-Net](model/unet.html) that predicts noise\n",
        "         $\\epsilon_\\text{cond}(x_t, c)$, in latent space\n",
        "        :param autoencoder: is the [AutoEncoder](model/autoencoder.html)\n",
        "        :param clip_embedder: is the [CLIP embeddings generator](model/clip_embedder.html)\n",
        "        :param latent_scaling_factor: is the scaling factor for the latent space. The encodings of\n",
        "         the autoencoder are scaled by this before feeding into the U-Net.\n",
        "        :param n_steps: is the number of diffusion steps $T$.\n",
        "        :param linear_start: is the start of the $\\beta$ schedule.\n",
        "        :param linear_end: is the end of the $\\beta$ schedule.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Wrap the [U-Net](model/unet.html) to keep the same model structure as\n",
        "        # [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion).\n",
        "        self.model = DiffusionWrapper(unet_model)\n",
        "        # Auto-encoder and scaling factor\n",
        "        self.first_stage_model = autoencoder\n",
        "        self.latent_scaling_factor = latent_scaling_factor\n",
        "        # [CLIP embeddings generator](model/clip_embedder.html)\n",
        "        self.cond_stage_model = clip_embedder\n",
        "\n",
        "        # Number of steps $T$\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "        # $\\beta$ schedule\n",
        "        beta = torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_steps, dtype=torch.float64) ** 2\n",
        "        self.beta = nn.Parameter(beta.to(torch.float32), requires_grad=False)\n",
        "        # $\\alpha_t = 1 - \\beta_t$\n",
        "        alpha = 1. - beta\n",
        "        # $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n",
        "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
        "        self.alpha_bar = nn.Parameter(alpha_bar.to(torch.float32), requires_grad=False)\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        \"\"\"\n",
        "        ### Get model device\n",
        "        \"\"\"\n",
        "        return next(iter(self.model.parameters())).device\n",
        "\n",
        "    def get_text_conditioning(self, prompts: List[str]):\n",
        "        \"\"\"\n",
        "        ### Get [CLIP embeddings](model/clip_embedder.html) for a list of text prompts\n",
        "        \"\"\"\n",
        "        return self.cond_stage_model(prompts)\n",
        "\n",
        "    def autoencoder_encode(self, image: torch.Tensor):\n",
        "        \"\"\"\n",
        "        ### Get scaled latent space representation of the image\n",
        "\n",
        "        The encoder output is a distribution.\n",
        "        We sample from that and multiply by the scaling factor.\n",
        "        \"\"\"\n",
        "        return self.latent_scaling_factor * self.first_stage_model.encode(image).sample()\n",
        "\n",
        "    def autoencoder_decode(self, z: torch.Tensor):\n",
        "        \"\"\"\n",
        "        ### Get image from the latent representation\n",
        "\n",
        "        We scale down by the scaling factor and then decode.\n",
        "        \"\"\"\n",
        "        return self.first_stage_model.decode(z / self.latent_scaling_factor)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor, context: torch.Tensor):\n",
        "        \"\"\"\n",
        "        ### Predict noise\n",
        "\n",
        "        Predict noise given the latent representation $x_t$, time step $t$, and the\n",
        "        conditioning context $c$.\n",
        "\n",
        "        $$\\epsilon_\\text{cond}(x_t, c)$$\n",
        "        \"\"\"\n",
        "        return self.model(x, t, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "EbJiPF7UnmNk"
      },
      "outputs": [],
      "source": [
        "#@title DiffusionSampler\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "\n",
        "class DiffusionSampler:\n",
        "    \"\"\"\n",
        "    ## Base class for sampling algorithms\n",
        "    \"\"\"\n",
        "    model: LatentDiffusion\n",
        "\n",
        "    def __init__(self, model: LatentDiffusion):\n",
        "        \"\"\"\n",
        "        :param model: is the model to predict noise $\\epsilon_\\text{cond}(x_t, c)$\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Set the model $\\epsilon_\\text{cond}(x_t, c)$\n",
        "        self.model = model\n",
        "        # Get number of steps the model was trained with $T$\n",
        "        self.n_steps = model.n_steps\n",
        "\n",
        "    def get_eps(self, x: torch.Tensor, t: torch.Tensor, c: torch.Tensor, *,\n",
        "                uncond_scale: float, uncond_cond: Optional[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        ## Get $\\epsilon(x_t, c)$\n",
        "\n",
        "        :param x: is $x_t$ of shape `[batch_size, channels, height, width]`\n",
        "        :param t: is $t$ of shape `[batch_size]`\n",
        "        :param c: is the conditional embeddings $c$ of shape `[batch_size, emb_size]`\n",
        "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
        "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
        "        \"\"\"\n",
        "        # When the scale $s = 1$\n",
        "        # $$\\epsilon_\\theta(x_t, c) = \\epsilon_\\text{cond}(x_t, c)$$\n",
        "        if uncond_cond is None or uncond_scale == 1.:\n",
        "            return self.model(x, t, c)\n",
        "\n",
        "        # Duplicate $x_t$ and $t$\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        t_in = torch.cat([t] * 2)\n",
        "        # Concatenated $c$ and $c_u$\n",
        "        c_in = torch.cat([uncond_cond, c])\n",
        "        # Get $\\epsilon_\\text{cond}(x_t, c)$ and $\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        e_t_uncond, e_t_cond = self.model(x_in, t_in, c_in).chunk(2)\n",
        "        # Calculate\n",
        "        # $$\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$$\n",
        "        e_t = e_t_uncond + uncond_scale * (e_t_cond - e_t_uncond)\n",
        "\n",
        "        #\n",
        "        return e_t\n",
        "\n",
        "    def sample(self,\n",
        "               shape: List[int],\n",
        "               cond: torch.Tensor,\n",
        "               repeat_noise: bool = False,\n",
        "               temperature: float = 1.,\n",
        "               x_last: Optional[torch.Tensor] = None,\n",
        "               uncond_scale: float = 1.,\n",
        "               uncond_cond: Optional[torch.Tensor] = None,\n",
        "               skip_steps: int = 0,\n",
        "               ):\n",
        "        \"\"\"\n",
        "        ### Sampling Loop\n",
        "\n",
        "        :param shape: is the shape of the generated images in the\n",
        "            form `[batch_size, channels, height, width]`\n",
        "        :param cond: is the conditional embeddings $c$\n",
        "        :param temperature: is the noise temperature (random noise gets multiplied by this)\n",
        "        :param x_last: is $x_T$. If not provided random noise will be used.\n",
        "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
        "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
        "        :param skip_steps: is the number of time steps to skip.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def paint(self, x: torch.Tensor, cond: torch.Tensor, t_start: int, *,\n",
        "              orig: Optional[torch.Tensor] = None,\n",
        "              mask: Optional[torch.Tensor] = None, orig_noise: Optional[torch.Tensor] = None,\n",
        "              uncond_scale: float = 1.,\n",
        "              uncond_cond: Optional[torch.Tensor] = None,\n",
        "              ):\n",
        "        \"\"\"\n",
        "        ### Painting Loop\n",
        "\n",
        "        :param x: is $x_{T'}$ of shape `[batch_size, channels, height, width]`\n",
        "        :param cond: is the conditional embeddings $c$\n",
        "        :param t_start: is the sampling step to start from, $T'$\n",
        "        :param orig: is the original image in latent page which we are in paining.\n",
        "        :param mask: is the mask to keep the original image.\n",
        "        :param orig_noise: is fixed noise to be added to the original image.\n",
        "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
        "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def q_sample(self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        ### Sample from $q(x_t|x_0)$\n",
        "\n",
        "        :param x0: is $x_0$ of shape `[batch_size, channels, height, width]`\n",
        "        :param index: is the time step $t$ index\n",
        "        :param noise: is the noise, $\\epsilon$\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "_VbiQo2xm_ew"
      },
      "outputs": [],
      "source": [
        "#@title DDIMSampler\n",
        "\n",
        "from typing import Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class DDIMSampler(DiffusionSampler):\n",
        "    \"\"\"\n",
        "    ## DDIM Sampler\n",
        "\n",
        "    This extends the [`DiffusionSampler` base class](index.html).\n",
        "\n",
        "    DDIM samples images by repeatedly removing noise by sampling step by step using,\n",
        "\n",
        "    \\begin{align}\n",
        "    x_{\\tau_{i-1}} &= \\sqrt{\\alpha_{\\tau_{i-1}}}\\Bigg(\n",
        "            \\frac{x_{\\tau_i} - \\sqrt{1 - \\alpha_{\\tau_i}}\\epsilon_\\theta(x_{\\tau_i})}{\\sqrt{\\alpha_{\\tau_i}}}\n",
        "            \\Bigg) \\\\\n",
        "            &+ \\sqrt{1 - \\alpha_{\\tau_{i- 1}} - \\sigma_{\\tau_i}^2} \\cdot \\epsilon_\\theta(x_{\\tau_i}) \\\\\n",
        "            &+ \\sigma_{\\tau_i} \\epsilon_{\\tau_i}\n",
        "    \\end{align}\n",
        "\n",
        "    where $\\epsilon_{\\tau_i}$ is random noise,\n",
        "    $\\tau$ is a subsequence of $[1,2,\\dots,T]$ of length $S$,\n",
        "    and\n",
        "    $$\\sigma_{\\tau_i} =\n",
        "    \\eta \\sqrt{\\frac{1 - \\alpha_{\\tau_{i-1}}}{1 - \\alpha_{\\tau_i}}}\n",
        "    \\sqrt{1 - \\frac{\\alpha_{\\tau_i}}{\\alpha_{\\tau_{i-1}}}}$$\n",
        "\n",
        "    Note that, $\\alpha_t$ in DDIM paper refers to ${\\color{lightgreen}\\bar\\alpha_t}$ from [DDPM](ddpm.html).\n",
        "    \"\"\"\n",
        "\n",
        "    model: LatentDiffusion\n",
        "\n",
        "    def __init__(self, model: LatentDiffusion, n_steps: int, ddim_discretize: str = \"uniform\", ddim_eta: float = 0.):\n",
        "        \"\"\"\n",
        "        :param model: is the model to predict noise $\\epsilon_\\text{cond}(x_t, c)$\n",
        "        :param n_steps: is the number of DDIM sampling steps, $S$\n",
        "        :param ddim_discretize: specifies how to extract $\\tau$ from $[1,2,\\dots,T]$.\n",
        "            It can be either `uniform` or `quad`.\n",
        "        :param ddim_eta: is $\\eta$ used to calculate $\\sigma_{\\tau_i}$. $\\eta = 0$ makes the\n",
        "            sampling process deterministic.\n",
        "        \"\"\"\n",
        "        super().__init__(model)\n",
        "        # Number of steps, $T$\n",
        "        self.n_steps = model.n_steps\n",
        "\n",
        "        # Calculate $\\tau$ to be uniformly distributed across $[1,2,\\dots,T]$\n",
        "        if ddim_discretize == 'uniform':\n",
        "            c = self.n_steps // n_steps\n",
        "            self.time_steps = np.asarray(list(range(0, self.n_steps, c))) + 1\n",
        "        # Calculate $\\tau$ to be quadratically distributed across $[1,2,\\dots,T]$\n",
        "        elif ddim_discretize == 'quad':\n",
        "            self.time_steps = ((np.linspace(0, np.sqrt(self.n_steps * .8), n_steps)) ** 2).astype(int) + 1\n",
        "        else:\n",
        "            raise NotImplementedError(ddim_discretize)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get ${\\color{lightgreen}\\bar\\alpha_t}$\n",
        "            alpha_bar = self.model.alpha_bar\n",
        "\n",
        "            # $\\alpha_{\\tau_i}$\n",
        "            self.ddim_alpha = alpha_bar[self.time_steps].clone().to(torch.float32)\n",
        "            # $\\sqrt{\\alpha_{\\tau_i}}$\n",
        "            self.ddim_alpha_sqrt = torch.sqrt(self.ddim_alpha)\n",
        "            # $\\alpha_{\\tau_{i-1}}$\n",
        "            self.ddim_alpha_prev = torch.cat([alpha_bar[0:1], alpha_bar[self.time_steps[:-1]]])\n",
        "\n",
        "            # $$\\sigma_{\\tau_i} =\n",
        "            # \\eta \\sqrt{\\frac{1 - \\alpha_{\\tau_{i-1}}}{1 - \\alpha_{\\tau_i}}}\n",
        "            # \\sqrt{1 - \\frac{\\alpha_{\\tau_i}}{\\alpha_{\\tau_{i-1}}}}$$\n",
        "            self.ddim_sigma = (ddim_eta *\n",
        "                               ((1 - self.ddim_alpha_prev) / (1 - self.ddim_alpha) *\n",
        "                                (1 - self.ddim_alpha / self.ddim_alpha_prev)) ** .5)\n",
        "\n",
        "            # $\\sqrt{1 - \\alpha_{\\tau_i}}$\n",
        "            self.ddim_sqrt_one_minus_alpha = (1. - self.ddim_alpha) ** .5\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self,\n",
        "               shape: List[int],\n",
        "               cond: torch.Tensor,\n",
        "               repeat_noise: bool = False,\n",
        "               temperature: float = 1.,\n",
        "               x_last: Optional[torch.Tensor] = None,\n",
        "               uncond_scale: float = 1.,\n",
        "               uncond_cond: Optional[torch.Tensor] = None,\n",
        "               skip_steps: int = 0,\n",
        "               ):\n",
        "        \"\"\"\n",
        "        ### Sampling Loop\n",
        "\n",
        "        :param shape: is the shape of the generated images in the\n",
        "            form `[batch_size, channels, height, width]`\n",
        "        :param cond: is the conditional embeddings $c$\n",
        "        :param temperature: is the noise temperature (random noise gets multiplied by this)\n",
        "        :param x_last: is $x_{\\tau_S}$. If not provided random noise will be used.\n",
        "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
        "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
        "        :param skip_steps: is the number of time steps to skip $i'$. We start sampling from $S - i'$.\n",
        "            And `x_last` is then $x_{\\tau_{S - i'}}$.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get device and batch size\n",
        "        device = self.model.device\n",
        "        bs = shape[0]\n",
        "\n",
        "        # Get $x_{\\tau_S}$\n",
        "        x = x_last if x_last is not None else torch.randn(shape, device=device)\n",
        "\n",
        "        # Time steps to sample at $\\tau_{S - i'}, \\tau_{S - i' - 1}, \\dots, \\tau_1$\n",
        "        time_steps = np.flip(self.time_steps)[skip_steps:]\n",
        "\n",
        "        for i, step in enumerate(time_steps):\n",
        "            # Index $i$ in the list $[\\tau_1, \\tau_2, \\dots, \\tau_S]$\n",
        "            index = len(time_steps) - i - 1\n",
        "            # Time step $\\tau_i$\n",
        "            ts = x.new_full((bs,), step, dtype=torch.long)\n",
        "\n",
        "            # Sample $x_{\\tau_{i-1}}$\n",
        "            x, pred_x0, e_t = self.p_sample(x, cond, ts, step, index=index,\n",
        "                                            repeat_noise=repeat_noise,\n",
        "                                            temperature=temperature,\n",
        "                                            uncond_scale=uncond_scale,\n",
        "                                            uncond_cond=uncond_cond)\n",
        "\n",
        "        # Return $x_0$\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x: torch.Tensor, c: torch.Tensor, t: torch.Tensor, step: int, index: int, *,\n",
        "                 repeat_noise: bool = False,\n",
        "                 temperature: float = 1.,\n",
        "                 uncond_scale: float = 1.,\n",
        "                 uncond_cond: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        ### Sample $x_{\\tau_{i-1}}$\n",
        "\n",
        "        :param x: is $x_{\\tau_i}$ of shape `[batch_size, channels, height, width]`\n",
        "        :param c: is the conditional embeddings $c$ of shape `[batch_size, emb_size]`\n",
        "        :param t: is $\\tau_i$ of shape `[batch_size]`\n",
        "        :param step: is the step $\\tau_i$ as an integer\n",
        "        :param index: is index $i$ in the list $[\\tau_1, \\tau_2, \\dots, \\tau_S]$\n",
        "        :param repeat_noise: specified whether the noise should be same for all samples in the batch\n",
        "        :param temperature: is the noise temperature (random noise gets multiplied by this)\n",
        "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
        "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
        "        \"\"\"\n",
        "\n",
        "        # Get $\\epsilon_\\theta(x_{\\tau_i})$\n",
        "        e_t = self.get_eps(x, t, c,\n",
        "                           uncond_scale=uncond_scale,\n",
        "                           uncond_cond=uncond_cond)\n",
        "\n",
        "        # Calculate $x_{\\tau_{i - 1}}$ and predicted $x_0$\n",
        "        x_prev, pred_x0 = self.get_x_prev_and_pred_x0(e_t, index, x,\n",
        "                                                      temperature=temperature,\n",
        "                                                      repeat_noise=repeat_noise)\n",
        "\n",
        "        #\n",
        "        return x_prev, pred_x0, e_t\n",
        "\n",
        "    def get_x_prev_and_pred_x0(self, e_t: torch.Tensor, index: int, x: torch.Tensor, *,\n",
        "                               temperature: float,\n",
        "                               repeat_noise: bool):\n",
        "        \"\"\"\n",
        "        ### Sample $x_{\\tau_{i-1}}$ given $\\epsilon_\\theta(x_{\\tau_i})$\n",
        "        \"\"\"\n",
        "\n",
        "        # $\\alpha_{\\tau_i}$\n",
        "        alpha = self.ddim_alpha[index]\n",
        "        # $\\alpha_{\\tau_{i-1}}$\n",
        "        alpha_prev = self.ddim_alpha_prev[index]\n",
        "        # $\\sigma_{\\tau_i}$\n",
        "        sigma = self.ddim_sigma[index]\n",
        "        # $\\sqrt{1 - \\alpha_{\\tau_i}}$\n",
        "        sqrt_one_minus_alpha = self.ddim_sqrt_one_minus_alpha[index]\n",
        "\n",
        "        # Current prediction for $x_0$,\n",
        "        # $$\\frac{x_{\\tau_i} - \\sqrt{1 - \\alpha_{\\tau_i}}\\epsilon_\\theta(x_{\\tau_i})}{\\sqrt{\\alpha_{\\tau_i}}}$$\n",
        "        pred_x0 = (x - sqrt_one_minus_alpha * e_t) / (alpha ** 0.5)\n",
        "        # Direction pointing to $x_t$\n",
        "        # $$\\sqrt{1 - \\alpha_{\\tau_{i- 1}} - \\sigma_{\\tau_i}^2} \\cdot \\epsilon_\\theta(x_{\\tau_i})$$\n",
        "        dir_xt = (1. - alpha_prev - sigma ** 2).sqrt() * e_t\n",
        "\n",
        "        # No noise is added, when $\\eta = 0$\n",
        "        if sigma == 0.:\n",
        "            noise = 0.\n",
        "        # If same noise is used for all samples in the batch\n",
        "        elif repeat_noise:\n",
        "            noise = torch.randn((1, *x.shape[1:]), device=x.device)\n",
        "            # Different noise for each sample\n",
        "        else:\n",
        "            noise = torch.randn(x.shape, device=x.device)\n",
        "\n",
        "        # Multiply noise by the temperature\n",
        "        noise = noise * temperature\n",
        "\n",
        "        #  \\begin{align}\n",
        "        #     x_{\\tau_{i-1}} &= \\sqrt{\\alpha_{\\tau_{i-1}}}\\Bigg(\n",
        "        #             \\frac{x_{\\tau_i} - \\sqrt{1 - \\alpha_{\\tau_i}}\\epsilon_\\theta(x_{\\tau_i})}{\\sqrt{\\alpha_{\\tau_i}}}\n",
        "        #             \\Bigg) \\\\\n",
        "        #             &+ \\sqrt{1 - \\alpha_{\\tau_{i- 1}} - \\sigma_{\\tau_i}^2} \\cdot \\epsilon_\\theta(x_{\\tau_i}) \\\\\n",
        "        #             &+ \\sigma_{\\tau_i} \\epsilon_{\\tau_i}\n",
        "        #  \\end{align}\n",
        "        x_prev = (alpha_prev ** 0.5) * pred_x0 + dir_xt + sigma * noise\n",
        "\n",
        "        #\n",
        "        return x_prev, pred_x0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def q_sample(self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        ### Sample from $q_{\\sigma,\\tau}(x_{\\tau_i}|x_0)$\n",
        "\n",
        "        $$q_{\\sigma,\\tau}(x_t|x_0) =\n",
        "         \\mathcal{N} \\Big(x_t; \\sqrt{\\alpha_{\\tau_i}} x_0, (1-\\alpha_{\\tau_i}) \\mathbf{I} \\Big)$$\n",
        "\n",
        "        :param x0: is $x_0$ of shape `[batch_size, channels, height, width]`\n",
        "        :param index: is the time step $\\tau_i$ index $i$\n",
        "        :param noise: is the noise, $\\epsilon$\n",
        "        \"\"\"\n",
        "\n",
        "        # Random noise, if noise is not specified\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "\n",
        "        # Sample from\n",
        "        #  $$q_{\\sigma,\\tau}(x_t|x_0) =\n",
        "        #          \\mathcal{N} \\Big(x_t; \\sqrt{\\alpha_{\\tau_i}} x_0, (1-\\alpha_{\\tau_i}) \\mathbf{I} \\Big)$$\n",
        "        return self.ddim_alpha_sqrt[index] * x0 + self.ddim_sqrt_one_minus_alpha[index] * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def paint(self, x: torch.Tensor, cond: torch.Tensor, t_start: int, *,\n",
        "              orig: Optional[torch.Tensor] = None,\n",
        "              mask: Optional[torch.Tensor] = None, orig_noise: Optional[torch.Tensor] = None,\n",
        "              uncond_scale: float = 1.,\n",
        "              uncond_cond: Optional[torch.Tensor] = None,\n",
        "              ):\n",
        "        \"\"\"\n",
        "        ### Painting Loop\n",
        "\n",
        "        :param x: is $x_{S'}$ of shape `[batch_size, channels, height, width]`\n",
        "        :param cond: is the conditional embeddings $c$\n",
        "        :param t_start: is the sampling step to start from, $S'$\n",
        "        :param orig: is the original image in latent page which we are in paining.\n",
        "            If this is not provided, it'll be an image to image transformation.\n",
        "        :param mask: is the mask to keep the original image.\n",
        "        :param orig_noise: is fixed noise to be added to the original image.\n",
        "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
        "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
        "        \"\"\"\n",
        "        # Get  batch size\n",
        "        bs = x.shape[0]\n",
        "\n",
        "        # Time steps to sample at $\\tau_{S`}, \\tau_{S' - 1}, \\dots, \\tau_1$\n",
        "        time_steps = np.flip(self.time_steps[:t_start])\n",
        "\n",
        "        for i, step in monit.enum('Paint', time_steps):\n",
        "            # Index $i$ in the list $[\\tau_1, \\tau_2, \\dots, \\tau_S]$\n",
        "            index = len(time_steps) - i - 1\n",
        "            # Time step $\\tau_i$\n",
        "            ts = x.new_full((bs,), step, dtype=torch.long)\n",
        "\n",
        "            # Sample $x_{\\tau_{i-1}}$\n",
        "            x, _, _ = self.p_sample(x, cond, ts, step, index=index,\n",
        "                                    uncond_scale=uncond_scale,\n",
        "                                    uncond_cond=uncond_cond)\n",
        "\n",
        "            # Replace the masked area with original image\n",
        "            if orig is not None:\n",
        "                # Get the $q_{\\sigma,\\tau}(x_{\\tau_i}|x_0)$ for original image in latent space\n",
        "                orig_t = self.q_sample(orig, index, noise=orig_noise)\n",
        "                # Replace the masked area\n",
        "                x = orig_t * mask + x * (1 - mask)\n",
        "\n",
        "        #\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "vCpuTnxHoSiz"
      },
      "outputs": [],
      "source": [
        "#@title Util\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    ### Set random seeds\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_model(path: Path = None) -> LatentDiffusion:\n",
        "    \"\"\"\n",
        "    ### Load [`LatentDiffusion` model](latent_diffusion.html)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the autoencoder\n",
        "    print('Initialize autoencoder')\n",
        "    encoder = Encoder(z_channels=4,\n",
        "                      in_channels=3,\n",
        "                      channels=128,\n",
        "                      channel_multipliers=[1, 2, 4, 4],\n",
        "                      n_resnet_blocks=2)\n",
        "\n",
        "    decoder = Decoder(out_channels=3,\n",
        "                      z_channels=4,\n",
        "                      channels=128,\n",
        "                      channel_multipliers=[1, 2, 4, 4],\n",
        "                      n_resnet_blocks=2)\n",
        "\n",
        "    autoencoder = Autoencoder(emb_channels=4,\n",
        "                              encoder=encoder,\n",
        "                              decoder=decoder,\n",
        "                              z_channels=4)\n",
        "\n",
        "    # Initialize the CLIP text embedder\n",
        "    print('Initialize CLIP Embedder')\n",
        "    clip_text_embedder = CLIPTextEmbedder()\n",
        "\n",
        "    # Initialize the U-Net\n",
        "    print('Initialize U-Net')\n",
        "    unet_model = UNetModel(in_channels=4,\n",
        "                            out_channels=4,\n",
        "                            channels=320,\n",
        "                            attention_levels=[0, 1, 2],\n",
        "                            n_res_blocks=2,\n",
        "                            channel_multipliers=[1, 2, 4, 4],\n",
        "                            n_heads=8,\n",
        "                            tf_layers=1,\n",
        "                            d_cond=768)\n",
        "\n",
        "    # Initialize the Latent Diffusion model\n",
        "    print('Initialize Latent Diffusion model')\n",
        "    model = LatentDiffusion(linear_start=0.00085,\n",
        "                            linear_end=0.0120,\n",
        "                            n_steps=1000,\n",
        "                            latent_scaling_factor=0.18215,\n",
        "                            autoencoder=autoencoder,\n",
        "                            clip_embedder=clip_text_embedder,\n",
        "                            unet_model=unet_model)\n",
        "\n",
        "    # Load the checkpoint\n",
        "    print(f\"Loading model from {path}\")\n",
        "    checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "\n",
        "    # Set model state\n",
        "    print('Load state')\n",
        "    missing_keys, extra_keys = model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
        "\n",
        "    # Debugging output\n",
        "    print(missing_keys)\n",
        "    print(extra_keys)\n",
        "\n",
        "    #\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_img(path: str):\n",
        "    \"\"\"\n",
        "    ### Load an image\n",
        "\n",
        "    This loads an image from a file and returns a PyTorch tensor.\n",
        "\n",
        "    :param path: is the path of the image\n",
        "    \"\"\"\n",
        "    # Open Image\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    # Get image size\n",
        "    w, h = image.size\n",
        "    # Resize to a multiple of 32\n",
        "    w = w - w % 32\n",
        "    h = h - h % 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    # Convert to numpy and map to `[-1, 1]` for `[0, 255]`\n",
        "    image = np.array(image).astype(np.float32) * (2. / 255.0) - 1\n",
        "    # Transpose to shape `[batch_size, channels, height, width]`\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    # Convert to torch\n",
        "    return torch.from_numpy(image)\n",
        "\n",
        "\n",
        "def save_images(images: torch.Tensor, dest_path: str, prefix: str = '', img_format: str = 'jpeg'):\n",
        "    \"\"\"\n",
        "    ### Save a images\n",
        "\n",
        "    :param images: is the tensor with images of shape `[batch_size, channels, height, width]`\n",
        "    :param dest_path: is the folder to save images in\n",
        "    :param prefix: is the prefix to add to file names\n",
        "    :param img_format: is the image format\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the destination folder\n",
        "    os.makedirs(dest_path, exist_ok=True)\n",
        "\n",
        "    # Map images to `[0, 1]` space and clip\n",
        "    images = torch.clamp((images + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "    # Transpose to `[batch_size, height, width, channels]` and convert to numpy\n",
        "    images = images.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "    # Save images\n",
        "    for i, img in enumerate(images):\n",
        "        img = Image.fromarray((255. * img).astype(np.uint8))\n",
        "        img.save(os.path.join(dest_path, f\"{prefix}{i:05}.{img_format}\"), format=img_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "oBNApbeemgSh"
      },
      "outputs": [],
      "source": [
        "#@title Txt2Img\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "class Txt2Img:\n",
        "    \"\"\"\n",
        "    ### Text to image class\n",
        "    \"\"\"\n",
        "    model: LatentDiffusion\n",
        "\n",
        "    def __init__(self, *,\n",
        "                 checkpoint_path: Path,\n",
        "                 sampler_name: str,\n",
        "                 n_steps: int = 50,\n",
        "                 ddim_eta: float = 0.0,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param checkpoint_path: is the path of the checkpoint\n",
        "        :param sampler_name: is the name of the [sampler](../sampler/index.html)\n",
        "        :param n_steps: is the number of sampling steps\n",
        "        :param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\n",
        "        \"\"\"\n",
        "        # Load [latent diffusion model](../latent_diffusion.html)\n",
        "        self.model = load_model(checkpoint_path)\n",
        "        # Get device\n",
        "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        # Move the model to device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize [sampler](../sampler/index.html)\n",
        "        if sampler_name == 'ddim':\n",
        "            self.sampler = DDIMSampler(self.model,\n",
        "                                       n_steps=n_steps,\n",
        "                                       ddim_eta=ddim_eta)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, *,\n",
        "                 dest_path: str,\n",
        "                 batch_size: int = 3,\n",
        "                 prompt: str,\n",
        "                 h: int = 512, w: int = 512,\n",
        "                 uncond_scale: float = 7.5,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param dest_path: is the path to store the generated images\n",
        "        :param batch_size: is the number of images to generate in a batch\n",
        "        :param prompt: is the prompt to generate images with\n",
        "        :param h: is the height of the image\n",
        "        :param w: is the width of the image\n",
        "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
        "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
        "        \"\"\"\n",
        "        # Number of channels in the image\n",
        "        c = 4\n",
        "        # Image to latent space resolution reduction\n",
        "        f = 8\n",
        "\n",
        "        # Make a batch of prompts\n",
        "        prompts = batch_size * [prompt]\n",
        "\n",
        "        # AMP auto casting\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n",
        "            if uncond_scale != 1.0:\n",
        "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
        "            else:\n",
        "                un_cond = None\n",
        "            # Get the prompt embeddings\n",
        "            cond = self.model.get_text_conditioning(prompts)\n",
        "            # [Sample in the latent space](../sampler/index.html).\n",
        "            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n",
        "            x = self.sampler.sample(cond=cond,\n",
        "                                    shape=[batch_size, c, h // f, w // f],\n",
        "                                    uncond_scale=uncond_scale,\n",
        "                                    uncond_cond=un_cond)\n",
        "            # Decode the image from the [autoencoder](../model/autoencoder.html)\n",
        "            images = self.model.autoencoder_decode(x)\n",
        "\n",
        "        # Save images\n",
        "        save_images(images, dest_path, 'txt_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xMMx3VQ6sGMI"
      },
      "outputs": [],
      "source": [
        "#@title Load Model\n",
        "import gc\n",
        "import torch\n",
        "txt2img = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "CrossAttention.use_flash_attention = False\n",
        "txt2img = Txt2Img(checkpoint_path='/content/sd-v1-1.ckpt', sampler_name='ddim', n_steps=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zwD1CxPQvjHa"
      },
      "outputs": [],
      "source": [
        "set_seed(40)\n",
        "txt2img(dest_path='outputs', batch_size=1, prompt='dog', uncond_scale=7.5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
