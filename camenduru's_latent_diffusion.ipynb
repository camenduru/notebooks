{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/notebooks/blob/main/camenduru's_latent_diffusion_latex.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xxXZPslehgp8"
      },
      "outputs": [],
      "source": [
        "#@title Install & Download Model\n",
        "\n",
        "#from https://github.com/labmlai/annotated_deep_learning_paper_implementations\n",
        "!pip install -q transformers pytorch_lightning\n",
        "!wget https://huggingface.co/CompVis/stable-diffusion-v-1-1-original/resolve/main/sd-v1-1.ckpt -O /content/sd-v1-1.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yanTNeiCivET"
      },
      "outputs": [],
      "source": [
        "#@title Autoencoder\n",
        "\n",
        "from typing import List\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoder: 'Encoder', decoder: 'Decoder', emb_channels: int, z_channels: int):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.quant_conv = nn.Conv2d(2 * z_channels, 2 * emb_channels, 1)\n",
        "        self.post_quant_conv = nn.Conv2d(emb_channels, z_channels, 1)\n",
        "\n",
        "    def encode(self, img: torch.Tensor) -> 'GaussianDistribution':\n",
        "        z = self.encoder(img)\n",
        "        moments = self.quant_conv(z)\n",
        "        return GaussianDistribution(moments)\n",
        "\n",
        "    def decode(self, z: torch.Tensor):\n",
        "        z = self.post_quant_conv(z)\n",
        "        return self.decoder(z)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, channels: int, channel_multipliers: List[int], n_resnet_blocks: int,\n",
        "                 in_channels: int, z_channels: int):\n",
        "        super().__init__()\n",
        "        n_resolutions = len(channel_multipliers)\n",
        "        self.conv_in = nn.Conv2d(in_channels, channels, 3, stride=1, padding=1)\n",
        "        channels_list = [m * channels for m in [1] + channel_multipliers]\n",
        "        self.down = nn.ModuleList()\n",
        "        for i in range(n_resolutions):\n",
        "            resnet_blocks = nn.ModuleList()\n",
        "            for _ in range(n_resnet_blocks):\n",
        "                resnet_blocks.append(ResnetBlock(channels, channels_list[i + 1]))\n",
        "                channels = channels_list[i + 1]\n",
        "            down = nn.Module()\n",
        "            down.block = resnet_blocks\n",
        "            if i != n_resolutions - 1:\n",
        "                down.downsample = DownSample(channels)\n",
        "            else:\n",
        "                down.downsample = nn.Identity()\n",
        "            self.down.append(down)\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(channels, channels)\n",
        "        self.mid.attn_1 = AttnBlock(channels)\n",
        "        self.mid.block_2 = ResnetBlock(channels, channels)\n",
        "        self.norm_out = normalization(channels)\n",
        "        self.conv_out = nn.Conv2d(channels, 2 * z_channels, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, img: torch.Tensor):\n",
        "        x = self.conv_in(img)\n",
        "        for down in self.down:\n",
        "            for block in down.block:\n",
        "                x = block(x)\n",
        "            x = down.downsample(x)\n",
        "        x = self.mid.block_1(x)\n",
        "        x = self.mid.attn_1(x)\n",
        "        x = self.mid.block_2(x)\n",
        "        x = self.norm_out(x)\n",
        "        x = swish(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, channels: int, channel_multipliers: List[int], n_resnet_blocks: int,\n",
        "                 out_channels: int, z_channels: int):\n",
        "        super().__init__()\n",
        "        num_resolutions = len(channel_multipliers)\n",
        "        channels_list = [m * channels for m in channel_multipliers]\n",
        "        channels = channels_list[-1]\n",
        "        self.conv_in = nn.Conv2d(z_channels, channels, 3, stride=1, padding=1)\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(channels, channels)\n",
        "        self.mid.attn_1 = AttnBlock(channels)\n",
        "        self.mid.block_2 = ResnetBlock(channels, channels)\n",
        "        self.up = nn.ModuleList()\n",
        "        for i in reversed(range(num_resolutions)):\n",
        "            resnet_blocks = nn.ModuleList()\n",
        "            for _ in range(n_resnet_blocks + 1):\n",
        "                resnet_blocks.append(ResnetBlock(channels, channels_list[i]))\n",
        "                channels = channels_list[i]\n",
        "            up = nn.Module()\n",
        "            up.block = resnet_blocks\n",
        "            if i != 0:\n",
        "                up.upsample = UpSample(channels)\n",
        "            else:\n",
        "                up.upsample = nn.Identity()\n",
        "            self.up.insert(0, up)\n",
        "        self.norm_out = normalization(channels)\n",
        "        self.conv_out = nn.Conv2d(channels, out_channels, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z: torch.Tensor):\n",
        "        h = self.conv_in(z)\n",
        "        h = self.mid.block_1(h)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h)\n",
        "        for up in reversed(self.up):\n",
        "            for block in up.block:\n",
        "                h = block(h)\n",
        "            h = up.upsample(h)\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        img = self.conv_out(h)\n",
        "        return img\n",
        "\n",
        "class GaussianDistribution:\n",
        "    def __init__(self, parameters: torch.Tensor):\n",
        "        self.mean, log_var = torch.chunk(parameters, 2, dim=1)\n",
        "        self.log_var = torch.clamp(log_var, -30.0, 20.0)\n",
        "        self.std = torch.exp(0.5 * self.log_var)\n",
        "\n",
        "    def sample(self):\n",
        "        return self.mean + self.std * torch.randn_like(self.std)\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.norm = normalization(channels)\n",
        "        self.q = nn.Conv2d(channels, channels, 1)\n",
        "        self.k = nn.Conv2d(channels, channels, 1)\n",
        "        self.v = nn.Conv2d(channels, channels, 1)\n",
        "        self.proj_out = nn.Conv2d(channels, channels, 1)\n",
        "        self.scale = channels ** -0.5\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x_norm = self.norm(x)\n",
        "        q = self.q(x_norm)\n",
        "        k = self.k(x_norm)\n",
        "        v = self.v(x_norm)\n",
        "        b, c, h, w = q.shape\n",
        "        q = q.view(b, c, h * w)\n",
        "        k = k.view(b, c, h * w)\n",
        "        v = v.view(b, c, h * w)\n",
        "        attn = torch.einsum('bci,bcj->bij', q, k) * self.scale\n",
        "        attn = F.softmax(attn, dim=2)\n",
        "        out = torch.einsum('bij,bcj->bci', attn, v)\n",
        "        out = out.view(b, c, h, w)\n",
        "        out = self.proj_out(out)\n",
        "        return x + out\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        return self.conv(x)\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.pad(x, (0, 1, 0, 1), mode=\"constant\", value=0)\n",
        "        return self.conv(x)\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.norm1 = normalization(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1)\n",
        "        self.norm2 = normalization(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
        "        if in_channels != out_channels:\n",
        "            self.nin_shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0)\n",
        "        else:\n",
        "            self.nin_shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv1(h)\n",
        "        h = self.norm2(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv2(h)\n",
        "        return self.nin_shortcut(x) + h\n",
        "\n",
        "def swish(x: torch.Tensor):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "def normalization(channels: int):\n",
        "    return nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tJdi-CNei7RX"
      },
      "outputs": [],
      "source": [
        "#@title CLIPTextEmbedder\n",
        "\n",
        "from typing import List\n",
        "from torch import nn\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "\n",
        "class CLIPTextEmbedder(nn.Module):\n",
        "    def __init__(self, version: str = \"openai/clip-vit-large-patch14\", device=\"cuda:0\", max_length: int = 77):\n",
        "        super().__init__()\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
        "        self.transformer = CLIPTextModel.from_pretrained(version).eval()\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def forward(self, prompts: List[str]):\n",
        "        batch_encoding = self.tokenizer(prompts, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
        "        return self.transformer(input_ids=tokens).last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LWfE5L3hjzS0"
      },
      "outputs": [],
      "source": [
        "#@title SpatialTransformer | CrossAttention\n",
        "\n",
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, channels: int, n_heads: int, n_layers: int, d_cond: int):\n",
        "        super().__init__()\n",
        "        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n",
        "        self.proj_in = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(channels, n_heads, channels // n_heads, d_cond=d_cond) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.proj_out = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = x.permute(0, 2, 3, 1).view(b, h * w, c)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, cond)\n",
        "        x = x.view(b, h, w, c).permute(0, 3, 1, 2)\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_head: int, d_cond: int):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(d_model, d_model, n_heads, d_head)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.attn2 = CrossAttention(d_model, d_cond, n_heads, d_head)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), cond=cond) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    use_flash_attention: bool = False\n",
        "    def __init__(self, d_model: int, d_cond: int, n_heads: int, d_head: int, is_inplace: bool = True):\n",
        "        super().__init__()\n",
        "        self.is_inplace = is_inplace\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_head\n",
        "        self.scale = d_head ** -0.5\n",
        "        d_attn = d_head * n_heads\n",
        "        self.to_q = nn.Linear(d_model, d_attn, bias=False)\n",
        "        self.to_k = nn.Linear(d_cond, d_attn, bias=False)\n",
        "        self.to_v = nn.Linear(d_cond, d_attn, bias=False)\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_attn, d_model))\n",
        "        try:\n",
        "            from flash_attn.flash_attention import FlashAttention\n",
        "            self.flash = FlashAttention()\n",
        "            self.flash.softmax_scale = self.scale\n",
        "        except ImportError:\n",
        "            self.flash = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cond: Optional[torch.Tensor] = None):\n",
        "        has_cond = cond is not None\n",
        "        if not has_cond:\n",
        "            cond = x\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(cond)\n",
        "        v = self.to_v(cond)\n",
        "        if CrossAttention.use_flash_attention and self.flash is not None and not has_cond and self.d_head <= 128:\n",
        "            return self.flash_attention(q, k, v)\n",
        "        else:\n",
        "            return self.normal_attention(q, k, v)\n",
        "\n",
        "    def flash_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
        "        batch_size, seq_len, _ = q.shape\n",
        "        qkv = torch.stack((q, k, v), dim=2)\n",
        "        qkv = qkv.view(batch_size, seq_len, 3, self.n_heads, self.d_head)\n",
        "        if self.d_head <= 32:\n",
        "            pad = 32 - self.d_head\n",
        "        elif self.d_head <= 64:\n",
        "            pad = 64 - self.d_head\n",
        "        elif self.d_head <= 128:\n",
        "            pad = 128 - self.d_head\n",
        "        else:\n",
        "            raise ValueError(f'Head size ${self.d_head} too large for Flash Attention')\n",
        "        if pad:\n",
        "            qkv = torch.cat((qkv, qkv.new_zeros(batch_size, seq_len, 3, self.n_heads, pad)), dim=-1)\n",
        "        out, _ = self.flash(qkv)\n",
        "        out = out[:, :, :, :self.d_head]\n",
        "        out = out.reshape(batch_size, seq_len, self.n_heads * self.d_head)\n",
        "        return self.to_out(out)\n",
        "\n",
        "    def normal_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
        "        q = q.view(*q.shape[:2], self.n_heads, -1)\n",
        "        k = k.view(*k.shape[:2], self.n_heads, -1)\n",
        "        v = v.view(*v.shape[:2], self.n_heads, -1)\n",
        "        attn = torch.einsum('bihd,bjhd->bhij', q, k) * self.scale\n",
        "        if self.is_inplace:\n",
        "            half = attn.shape[0] // 2\n",
        "            attn[half:] = attn[half:].softmax(dim=-1)\n",
        "            attn[:half] = attn[:half].softmax(dim=-1)\n",
        "        else:\n",
        "            attn = attn.softmax(dim=-1)\n",
        "        out = torch.einsum('bhij,bjhd->bihd', attn, v)\n",
        "        out = out.reshape(*out.shape[:2], -1)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_mult: int = 4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            GeGLU(d_model, d_model * d_mult),\n",
        "            nn.Dropout(0.),\n",
        "            nn.Linear(d_model * d_mult, d_model)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.net(x)\n",
        "\n",
        "class GeGLU(nn.Module):\n",
        "    def __init__(self, d_in: int, d_out: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_in, d_out * 2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HQejRsDFi_y7"
      },
      "outputs": [],
      "source": [
        "#@title UNetModel\n",
        "\n",
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    def __init__(\n",
        "            self, *,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            channels: int,\n",
        "            n_res_blocks: int,\n",
        "            attention_levels: List[int],\n",
        "            channel_multipliers: List[int],\n",
        "            n_heads: int,\n",
        "            tf_layers: int = 1,\n",
        "            d_cond: int = 768):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        levels = len(channel_multipliers)\n",
        "        d_time_emb = channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(channels, d_time_emb),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(d_time_emb, d_time_emb),\n",
        "        )\n",
        "        self.input_blocks = nn.ModuleList()\n",
        "        self.input_blocks.append(TimestepEmbedSequential(\n",
        "            nn.Conv2d(in_channels, channels, 3, padding=1)))\n",
        "        input_block_channels = [channels]\n",
        "        channels_list = [channels * m for m in channel_multipliers]\n",
        "        for i in range(levels):\n",
        "            for _ in range(n_res_blocks):\n",
        "                layers = [ResBlock(channels, d_time_emb, out_channels=channels_list[i])]\n",
        "                channels = channels_list[i]\n",
        "                if i in attention_levels:\n",
        "                    layers.append(SpatialTransformer(channels, n_heads, tf_layers, d_cond))\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                input_block_channels.append(channels)\n",
        "            if i != levels - 1:\n",
        "                self.input_blocks.append(TimestepEmbedSequential(DownSample(channels)))\n",
        "                input_block_channels.append(channels)\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(channels, d_time_emb),\n",
        "            SpatialTransformer(channels, n_heads, tf_layers, d_cond),\n",
        "            ResBlock(channels, d_time_emb),\n",
        "        )\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for i in reversed(range(levels)):\n",
        "            for j in range(n_res_blocks + 1):\n",
        "                layers = [ResBlock(channels + input_block_channels.pop(), d_time_emb, out_channels=channels_list[i])]\n",
        "                channels = channels_list[i]\n",
        "                if i in attention_levels:\n",
        "                    layers.append(SpatialTransformer(channels, n_heads, tf_layers, d_cond))\n",
        "                if i != 0 and j == n_res_blocks:\n",
        "                    layers.append(UpSample(channels))\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "        self.out = nn.Sequential(\n",
        "            normalization(channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels, out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def time_step_embedding(self, time_steps: torch.Tensor, max_period: int = 10000):\n",
        "        half = self.channels // 2\n",
        "        frequencies = torch.exp(\n",
        "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "        ).to(device=time_steps.device)\n",
        "        args = time_steps[:, None].float() * frequencies[None]\n",
        "        return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, time_steps: torch.Tensor, cond: torch.Tensor):\n",
        "        x_input_block = []\n",
        "        t_emb = self.time_step_embedding(time_steps)\n",
        "        t_emb = self.time_embed(t_emb)\n",
        "        for module in self.input_blocks:\n",
        "            x = module(x, t_emb, cond)\n",
        "            x_input_block.append(x)\n",
        "        x = self.middle_block(x, t_emb, cond)\n",
        "        for module in self.output_blocks:\n",
        "            x = torch.cat([x, x_input_block.pop()], dim=1)\n",
        "            x = module(x, t_emb, cond)\n",
        "        return self.out(x)\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential):\n",
        "    def forward(self, x, t_emb, cond=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, ResBlock):\n",
        "                x = layer(x, t_emb)\n",
        "            elif isinstance(layer, SpatialTransformer):\n",
        "                x = layer(x, cond)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        return self.conv(x)\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.op = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.op(x)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels: int, d_t_emb: int, *, out_channels=None):\n",
        "        super().__init__()\n",
        "        if out_channels is None:\n",
        "            out_channels = channels\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization(channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels, out_channels, 3, padding=1),\n",
        "        )\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(d_t_emb, out_channels),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization(out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "        if out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        else:\n",
        "            self.skip_connection = nn.Conv2d(channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor):\n",
        "        h = self.in_layers(x)\n",
        "        t_emb = self.emb_layers(t_emb).type(h.dtype)\n",
        "        h = h + t_emb[:, :, None, None]\n",
        "        h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "def normalization(channels):\n",
        "    return GroupNorm32(32, channels)\n",
        "\n",
        "def _test_time_embeddings():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    m = UNetModel(in_channels=1, out_channels=1, channels=320, n_res_blocks=1, attention_levels=[],\n",
        "                  channel_multipliers=[],\n",
        "                  n_heads=1, tf_layers=1, d_cond=1)\n",
        "    te = m.time_step_embedding(torch.arange(0, 1000))\n",
        "    plt.plot(np.arange(1000), te[:, [50, 100, 190, 260]].numpy())\n",
        "    plt.legend([\"dim %d\" % p for p in [50, 100, 190, 260]])\n",
        "    plt.title(\"Time embeddings\")\n",
        "    plt.show()\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     _test_time_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3f_oamxVidgE"
      },
      "outputs": [],
      "source": [
        "#@title LatentDiffusion\n",
        "\n",
        "from typing import List\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DiffusionWrapper(nn.Module):\n",
        "    def __init__(self, diffusion_model: UNetModel):\n",
        "        super().__init__()\n",
        "        self.diffusion_model = diffusion_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor, time_steps: torch.Tensor, context: torch.Tensor):\n",
        "        return self.diffusion_model(x, time_steps, context)\n",
        "\n",
        "class LatentDiffusion(nn.Module):\n",
        "    model: DiffusionWrapper\n",
        "    first_stage_model: Autoencoder\n",
        "    cond_stage_model: CLIPTextEmbedder\n",
        "\n",
        "    def __init__(self,\n",
        "                 unet_model: UNetModel,\n",
        "                 autoencoder: Autoencoder,\n",
        "                 clip_embedder: CLIPTextEmbedder,\n",
        "                 latent_scaling_factor: float,\n",
        "                 n_steps: int,\n",
        "                 linear_start: float,\n",
        "                 linear_end: float,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.model = DiffusionWrapper(unet_model)\n",
        "        self.first_stage_model = autoencoder\n",
        "        self.latent_scaling_factor = latent_scaling_factor\n",
        "        self.cond_stage_model = clip_embedder\n",
        "        self.n_steps = n_steps\n",
        "        beta = torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_steps, dtype=torch.float64) ** 2\n",
        "        self.beta = nn.Parameter(beta.to(torch.float32), requires_grad=False)\n",
        "        alpha = 1. - beta\n",
        "        alpha_bar = torch.cumprod(alpha, dim=0)\n",
        "        self.alpha_bar = nn.Parameter(alpha_bar.to(torch.float32), requires_grad=False)\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(iter(self.model.parameters())).device\n",
        "\n",
        "    def get_text_conditioning(self, prompts: List[str]):\n",
        "        return self.cond_stage_model(prompts)\n",
        "\n",
        "    def autoencoder_encode(self, image: torch.Tensor):\n",
        "        return self.latent_scaling_factor * self.first_stage_model.encode(image).sample()\n",
        "\n",
        "    def autoencoder_decode(self, z: torch.Tensor):\n",
        "        return self.first_stage_model.decode(z / self.latent_scaling_factor)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor, context: torch.Tensor):\n",
        "        return self.model(x, t, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EbJiPF7UnmNk"
      },
      "outputs": [],
      "source": [
        "#@title DiffusionSampler\n",
        "\n",
        "from typing import Optional, List\n",
        "import torch\n",
        "\n",
        "class DiffusionSampler:\n",
        "    model: LatentDiffusion\n",
        "\n",
        "    def __init__(self, model: LatentDiffusion):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.n_steps = model.n_steps\n",
        "\n",
        "    def get_eps(self, x: torch.Tensor, t: torch.Tensor, c: torch.Tensor, *,\n",
        "                uncond_scale: float, uncond_cond: Optional[torch.Tensor]):\n",
        "        if uncond_cond is None or uncond_scale == 1.:\n",
        "            return self.model(x, t, c)\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        t_in = torch.cat([t] * 2)\n",
        "        c_in = torch.cat([uncond_cond, c])\n",
        "        e_t_uncond, e_t_cond = self.model(x_in, t_in, c_in).chunk(2)\n",
        "        e_t = e_t_uncond + uncond_scale * (e_t_cond - e_t_uncond)\n",
        "        return e_t\n",
        "\n",
        "    def sample(self,\n",
        "               shape: List[int],\n",
        "               cond: torch.Tensor,\n",
        "               repeat_noise: bool = False,\n",
        "               temperature: float = 1.,\n",
        "               x_last: Optional[torch.Tensor] = None,\n",
        "               uncond_scale: float = 1.,\n",
        "               uncond_cond: Optional[torch.Tensor] = None,\n",
        "               skip_steps: int = 0,\n",
        "               ):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def paint(self, x: torch.Tensor, cond: torch.Tensor, t_start: int, *,\n",
        "              orig: Optional[torch.Tensor] = None,\n",
        "              mask: Optional[torch.Tensor] = None, orig_noise: Optional[torch.Tensor] = None,\n",
        "              uncond_scale: float = 1.,\n",
        "              uncond_cond: Optional[torch.Tensor] = None,\n",
        "              ):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def q_sample(self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None):\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_VbiQo2xm_ew"
      },
      "outputs": [],
      "source": [
        "#@title DDIMSampler\n",
        "\n",
        "from typing import Optional, List\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class DDIMSampler(DiffusionSampler):\n",
        "    model: LatentDiffusion\n",
        "\n",
        "    def __init__(self, model: LatentDiffusion, n_steps: int, ddim_discretize: str = \"uniform\", ddim_eta: float = 0.):\n",
        "        super().__init__(model)\n",
        "        self.n_steps = model.n_steps\n",
        "        if ddim_discretize == 'uniform':\n",
        "            c = self.n_steps // n_steps\n",
        "            self.time_steps = np.asarray(list(range(0, self.n_steps, c))) + 1\n",
        "        elif ddim_discretize == 'quad':\n",
        "            self.time_steps = ((np.linspace(0, np.sqrt(self.n_steps * .8), n_steps)) ** 2).astype(int) + 1\n",
        "        else:\n",
        "            raise NotImplementedError(ddim_discretize)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            alpha_bar = self.model.alpha_bar\n",
        "            self.ddim_alpha = alpha_bar[self.time_steps].clone().to(torch.float32)\n",
        "            self.ddim_alpha_sqrt = torch.sqrt(self.ddim_alpha)\n",
        "            self.ddim_alpha_prev = torch.cat([alpha_bar[0:1], alpha_bar[self.time_steps[:-1]]])\n",
        "            self.ddim_sigma = (ddim_eta *\n",
        "                               ((1 - self.ddim_alpha_prev) / (1 - self.ddim_alpha) *\n",
        "                                (1 - self.ddim_alpha / self.ddim_alpha_prev)) ** .5)\n",
        "            self.ddim_sqrt_one_minus_alpha = (1. - self.ddim_alpha) ** .5\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self,\n",
        "               shape: List[int],\n",
        "               cond: torch.Tensor,\n",
        "               repeat_noise: bool = False,\n",
        "               temperature: float = 1.,\n",
        "               x_last: Optional[torch.Tensor] = None,\n",
        "               uncond_scale: float = 1.,\n",
        "               uncond_cond: Optional[torch.Tensor] = None,\n",
        "               skip_steps: int = 0,\n",
        "               ):\n",
        "        device = self.model.device\n",
        "        bs = shape[0]\n",
        "        x = x_last if x_last is not None else torch.randn(shape, device=device)\n",
        "        time_steps = np.flip(self.time_steps)[skip_steps:]\n",
        "\n",
        "        for i, step in enumerate(time_steps):\n",
        "            index = len(time_steps) - i - 1\n",
        "            ts = x.new_full((bs,), step, dtype=torch.long)\n",
        "            x, pred_x0, e_t = self.p_sample(x, cond, ts, step, index=index,\n",
        "                                            repeat_noise=repeat_noise,\n",
        "                                            temperature=temperature,\n",
        "                                            uncond_scale=uncond_scale,\n",
        "                                            uncond_cond=uncond_cond)\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x: torch.Tensor, c: torch.Tensor, t: torch.Tensor, step: int, index: int, *,\n",
        "                 repeat_noise: bool = False,\n",
        "                 temperature: float = 1.,\n",
        "                 uncond_scale: float = 1.,\n",
        "                 uncond_cond: Optional[torch.Tensor] = None):\n",
        "        e_t = self.get_eps(x, t, c,\n",
        "                           uncond_scale=uncond_scale,\n",
        "                           uncond_cond=uncond_cond)\n",
        "        x_prev, pred_x0 = self.get_x_prev_and_pred_x0(e_t, index, x,\n",
        "                                                      temperature=temperature,\n",
        "                                                      repeat_noise=repeat_noise)\n",
        "        return x_prev, pred_x0, e_t\n",
        "\n",
        "    def get_x_prev_and_pred_x0(self, e_t: torch.Tensor, index: int, x: torch.Tensor, *,\n",
        "                               temperature: float,\n",
        "                               repeat_noise: bool):\n",
        "        alpha = self.ddim_alpha[index]\n",
        "        alpha_prev = self.ddim_alpha_prev[index]\n",
        "        sigma = self.ddim_sigma[index]\n",
        "        sqrt_one_minus_alpha = self.ddim_sqrt_one_minus_alpha[index]\n",
        "        pred_x0 = (x - sqrt_one_minus_alpha * e_t) / (alpha ** 0.5)\n",
        "        dir_xt = (1. - alpha_prev - sigma ** 2).sqrt() * e_t\n",
        "        if sigma == 0.:\n",
        "            noise = 0.\n",
        "        elif repeat_noise:\n",
        "            noise = torch.randn((1, *x.shape[1:]), device=x.device)\n",
        "        else:\n",
        "            noise = torch.randn(x.shape, device=x.device)\n",
        "        noise = noise * temperature\n",
        "        x_prev = (alpha_prev ** 0.5) * pred_x0 + dir_xt + sigma * noise\n",
        "        return x_prev, pred_x0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def q_sample(self, x0: torch.Tensor, index: int, noise: Optional[torch.Tensor] = None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "        return self.ddim_alpha_sqrt[index] * x0 + self.ddim_sqrt_one_minus_alpha[index] * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def paint(self, x: torch.Tensor, cond: torch.Tensor, t_start: int, *,\n",
        "              orig: Optional[torch.Tensor] = None,\n",
        "              mask: Optional[torch.Tensor] = None, orig_noise: Optional[torch.Tensor] = None,\n",
        "              uncond_scale: float = 1.,\n",
        "              uncond_cond: Optional[torch.Tensor] = None,\n",
        "              ):\n",
        "        bs = x.shape[0]\n",
        "        time_steps = np.flip(self.time_steps[:t_start])\n",
        "        for i, step in monit.enum('Paint', time_steps):\n",
        "            index = len(time_steps) - i - 1\n",
        "            ts = x.new_full((bs,), step, dtype=torch.long)\n",
        "            x, _, _ = self.p_sample(x, cond, ts, step, index=index,\n",
        "                                    uncond_scale=uncond_scale,\n",
        "                                    uncond_cond=uncond_cond)\n",
        "            if orig is not None:\n",
        "                orig_t = self.q_sample(orig, index, noise=orig_noise)\n",
        "                x = orig_t * mask + x * (1 - mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vCpuTnxHoSiz"
      },
      "outputs": [],
      "source": [
        "#@title Util\n",
        "\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_model(path: Path = None) -> LatentDiffusion:\n",
        "    print('Initialize autoencoder')\n",
        "    encoder = Encoder(z_channels=4,\n",
        "                      in_channels=3,\n",
        "                      channels=128,\n",
        "                      channel_multipliers=[1, 2, 4, 4],\n",
        "                      n_resnet_blocks=2)\n",
        "    decoder = Decoder(out_channels=3,\n",
        "                      z_channels=4,\n",
        "                      channels=128,\n",
        "                      channel_multipliers=[1, 2, 4, 4],\n",
        "                      n_resnet_blocks=2)\n",
        "    autoencoder = Autoencoder(emb_channels=4,\n",
        "                              encoder=encoder,\n",
        "                              decoder=decoder,\n",
        "                              z_channels=4)\n",
        "    print('Initialize CLIP Embedder')\n",
        "    clip_text_embedder = CLIPTextEmbedder()\n",
        "    print('Initialize U-Net')\n",
        "    unet_model = UNetModel(in_channels=4,\n",
        "                            out_channels=4,\n",
        "                            channels=320,\n",
        "                            attention_levels=[0, 1, 2],\n",
        "                            n_res_blocks=2,\n",
        "                            channel_multipliers=[1, 2, 4, 4],\n",
        "                            n_heads=8,\n",
        "                            tf_layers=1,\n",
        "                            d_cond=768)\n",
        "    print('Initialize Latent Diffusion model')\n",
        "    model = LatentDiffusion(linear_start=0.00085,\n",
        "                            linear_end=0.0120,\n",
        "                            n_steps=1000,\n",
        "                            latent_scaling_factor=0.18215,\n",
        "                            autoencoder=autoencoder,\n",
        "                            clip_embedder=clip_text_embedder,\n",
        "                            unet_model=unet_model)\n",
        "    print(f\"Loading model from {path}\")\n",
        "    checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "    print('Load state')\n",
        "    missing_keys, extra_keys = model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
        "    print(missing_keys)\n",
        "    print(extra_keys)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_img(path: str):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    w = w - w % 32\n",
        "    h = h - h % 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) * (2. / 255.0) - 1\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    return torch.from_numpy(image)\n",
        "\n",
        "def save_images(images: torch.Tensor, dest_path: str, prefix: str = '', img_format: str = 'jpeg'):\n",
        "    os.makedirs(dest_path, exist_ok=True)\n",
        "    images = torch.clamp((images + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "    images = images.cpu().permute(0, 2, 3, 1).numpy()\n",
        "    for i, img in enumerate(images):\n",
        "        img = Image.fromarray((255. * img).astype(np.uint8))\n",
        "        img.save(os.path.join(dest_path, f\"{prefix}{i:05}.{img_format}\"), format=img_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oBNApbeemgSh"
      },
      "outputs": [],
      "source": [
        "#@title Txt2Img\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "class Txt2Img:\n",
        "    model: LatentDiffusion\n",
        "    def __init__(self, *,\n",
        "                 checkpoint_path: Path,\n",
        "                 sampler_name: str,\n",
        "                 n_steps: int = 50,\n",
        "                 ddim_eta: float = 0.0,\n",
        "                 ):\n",
        "        self.model = load_model(checkpoint_path)\n",
        "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        if sampler_name == 'ddim':\n",
        "            self.sampler = DDIMSampler(self.model,\n",
        "                                       n_steps=n_steps,\n",
        "                                       ddim_eta=ddim_eta)\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, *,\n",
        "                 dest_path: str,\n",
        "                 batch_size: int = 3,\n",
        "                 prompt: str,\n",
        "                 h: int = 512, w: int = 512,\n",
        "                 uncond_scale: float = 7.5,\n",
        "                 ):\n",
        "        c = 4\n",
        "        f = 8\n",
        "        prompts = batch_size * [prompt]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            if uncond_scale != 1.0:\n",
        "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
        "            else:\n",
        "                un_cond = None\n",
        "            cond = self.model.get_text_conditioning(prompts)\n",
        "            x = self.sampler.sample(cond=cond,\n",
        "                                    shape=[batch_size, c, h // f, w // f],\n",
        "                                    uncond_scale=uncond_scale,\n",
        "                                    uncond_cond=un_cond)\n",
        "            images = self.model.autoencoder_decode(x)\n",
        "        save_images(images, dest_path, 'txt_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xMMx3VQ6sGMI"
      },
      "outputs": [],
      "source": [
        "#@title Load Model\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "txt2img = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "CrossAttention.use_flash_attention = False\n",
        "txt2img = Txt2Img(checkpoint_path='/content/sd-v1-1.ckpt', sampler_name='ddim', n_steps=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwD1CxPQvjHa"
      },
      "outputs": [],
      "source": [
        "set_seed(40)\n",
        "txt2img(dest_path='outputs', batch_size=1, prompt='dog', uncond_scale=7.5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
