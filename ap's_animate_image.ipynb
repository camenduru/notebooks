{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/notebooks/blob/main/ap's_animate_image.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers accelerate transformers peft gradio \n",
    "!pip install -q https://download.pytorch.org/whl/cu118/xformers-0.0.22.post4%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "import random\n",
    "\n",
    "def generate_gif(image, animation_type):\n",
    "    # Load the motion adapter\n",
    "    adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n",
    "\n",
    "    # Load SD 1.5 based finetuned model\n",
    "    model_id = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\n",
    "    pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\n",
    "\n",
    "    # Scheduler setup\n",
    "    scheduler = DDIMScheduler(\n",
    "        clip_sample=False,\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"linear\",\n",
    "        timestep_spacing=\"trailing\",\n",
    "        steps_offset=1\n",
    "    )\n",
    "    pipe.scheduler = scheduler\n",
    "\n",
    "    # Enable memory savings\n",
    "    pipe.enable_vae_slicing()\n",
    "    pipe.enable_model_cpu_offload()\n",
    "\n",
    "    # Load ip_adapter\n",
    "    pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
    "\n",
    "    # Load the selected motion adapter\n",
    "    pipe.load_lora_weights(f\"guoyww/animatediff-motion-lora-{animation_type}\", adapter_name=animation_type)\n",
    "\n",
    "    # Generate a random seed\n",
    "    seed = random.randint(0, 2**32 - 1)\n",
    "    prompt = \"best quality, high quality, trending on artstation\"\n",
    "\n",
    "    # Set adapter weights for the selected adapter\n",
    "    adapter_weight = [0.75]\n",
    "\n",
    "    pipe.set_adapters([animation_type], adapter_weights=adapter_weight)\n",
    "\n",
    "    # Generate GIF\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        num_frames=16,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,\n",
    "        ip_adapter_image=image,\n",
    "        generator=torch.Generator(\"cpu\").manual_seed(seed),\n",
    "    )\n",
    "    frames = output.frames[0]\n",
    "\n",
    "    gif_path = \"output_animation.gif\"\n",
    "    export_to_gif(frames, gif_path)\n",
    "    return gif_path\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_gif,\n",
    "    inputs=[gr.Image(type=\"pil\"), gr.Radio([\"zoom-out\", \"tilt-up\", \"pan-left\"])],\n",
    "    outputs=gr.Image(type=\"pil\", label=\"Generated GIF\"),\n",
    "    title=\"AnimateDiff + IP Adapter Demo\",\n",
    "    description=\"Upload an image and select an motion module type to generate a GIF!\"\n",
    ")\n",
    "\n",
    "iface.launch(debug=True,share=True,inline=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
